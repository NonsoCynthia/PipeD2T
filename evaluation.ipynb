{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15007/2479653144.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "unseen_domains = ['Artist', 'Politician', 'CelestialBody', 'Athlete', 'MeanOfTransportation']\n",
    "# root path\n",
    "original_path = \"/home/cosuji/spinning-storage/cosuji/NLG_Exp/webnlg/\"\n",
    "path_input = original_path + 'data/deepnlg/input/'\n",
    "print(path_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Ordering and Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All domains:\n",
      "Task:  ordering\n",
      "Set:  dev\n",
      "Model:  random\n",
      "Accuracy:  0.29\n",
      "----------\n",
      "Task:  ordering\n",
      "Set:  dev\n",
      "Model:  major\n",
      "Accuracy:  0.54\n",
      "----------\n",
      "Task:  ordering\n",
      "Set:  dev\n",
      "Model:  transformer\n",
      "Accuracy:  0.59\n",
      "----------\n",
      "Task:  ordering\n",
      "Set:  dev\n",
      "Model:  rnn\n",
      "Accuracy:  0.63\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  dev\n",
      "Model:  random\n",
      "Accuracy:  0.26\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  dev\n",
      "Model:  major\n",
      "Accuracy:  0.49\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  dev\n",
      "Model:  transformer\n",
      "Accuracy:  0.65\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  dev\n",
      "Model:  rnn\n",
      "Accuracy:  0.67\n",
      "----------\n",
      "Task:  ordering\n",
      "Set:  test\n",
      "Model:  random\n",
      "Accuracy:  0.31\n",
      "----------\n",
      "Task:  ordering\n",
      "Set:  test\n",
      "Model:  major\n",
      "Accuracy:  0.48\n",
      "----------\n",
      "Task:  ordering\n",
      "Set:  test\n",
      "Model:  transformer\n",
      "Accuracy:  0.34\n",
      "----------\n",
      "Task:  ordering\n",
      "Set:  test\n",
      "Model:  rnn\n",
      "Accuracy:  0.35\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  test\n",
      "Model:  random\n",
      "Accuracy:  0.29\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  test\n",
      "Model:  major\n",
      "Accuracy:  0.27\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  test\n",
      "Model:  transformer\n",
      "Accuracy:  0.36\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  test\n",
      "Model:  rnn\n",
      "Accuracy:  0.39\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('All domains:')\n",
    "for _set in ['dev', 'test']:\n",
    "    for task in ['ordering', 'structing']:\n",
    "        gold_path=os.path.join(path, 'data', task, _set + '.json')\n",
    "        gold = json.load(open(gold_path))\n",
    "\n",
    "        for model in ['random', 'major', 'transformer', 'rnn']:\n",
    "            p = os.path.join(path, 'results/steps', task, model, _set + '.out.postprocessed')\n",
    "            with open(p) as f:\n",
    "                y_pred_ = f.read().split('\\n')[:-1]\n",
    "\n",
    "            y_real, y_pred = [], []\n",
    "            for i, g in enumerate(gold):\n",
    "                t = [' '.join(target['output']) for target in g['targets']]\n",
    "                y_real.append(t)\n",
    "                y_pred.append(y_pred_[i].strip())\n",
    "\n",
    "            num, dem = 0.0, 0\n",
    "            for i, y_ in enumerate(y_pred):\n",
    "                y = y_real[i]\n",
    "                if y_.strip() in y:\n",
    "                    num += 1\n",
    "                dem += 1\n",
    "            print('Task: ', task)\n",
    "            print('Set: ', _set)\n",
    "            print('Model: ', model)\n",
    "            print('Accuracy: ', round(num/dem, 2))\n",
    "            print(10 * '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen domains:\n",
      "Task:  ordering\n",
      "Set:  dev\n",
      "Model:  random\n",
      "Accuracy:  0.29\n",
      "----------\n",
      "Task:  ordering\n",
      "Set:  dev\n",
      "Model:  major\n",
      "Accuracy:  0.54\n",
      "----------\n",
      "Task:  ordering\n",
      "Set:  dev\n",
      "Model:  transformer\n",
      "Accuracy:  0.59\n",
      "----------\n",
      "Task:  ordering\n",
      "Set:  dev\n",
      "Model:  rnn\n",
      "Accuracy:  0.63\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  dev\n",
      "Model:  random\n",
      "Accuracy:  0.26\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  dev\n",
      "Model:  major\n",
      "Accuracy:  0.49\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  dev\n",
      "Model:  transformer\n",
      "Accuracy:  0.65\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  dev\n",
      "Model:  rnn\n",
      "Accuracy:  0.67\n",
      "----------\n",
      "Task:  ordering\n",
      "Set:  test\n",
      "Model:  random\n",
      "Accuracy:  0.29\n",
      "----------\n",
      "Task:  ordering\n",
      "Set:  test\n",
      "Model:  major\n",
      "Accuracy:  0.51\n",
      "----------\n",
      "Task:  ordering\n",
      "Set:  test\n",
      "Model:  transformer\n",
      "Accuracy:  0.56\n",
      "----------\n",
      "Task:  ordering\n",
      "Set:  test\n",
      "Model:  rnn\n",
      "Accuracy:  0.56\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  test\n",
      "Model:  random\n",
      "Accuracy:  0.29\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  test\n",
      "Model:  major\n",
      "Accuracy:  0.45\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  test\n",
      "Model:  transformer\n",
      "Accuracy:  0.59\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  test\n",
      "Model:  rnn\n",
      "Accuracy:  0.63\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('Seen domains:')\n",
    "for _set in ['dev', 'test']:\n",
    "    for task in ['ordering', 'structing']:\n",
    "        gold_path=os.path.join(path, 'data', task, _set + '.json')\n",
    "        gold = json.load(open(gold_path))\n",
    "\n",
    "        for model in ['random', 'major', 'transformer', 'rnn']:\n",
    "            p = os.path.join(path, 'results/steps', task, model, _set + '.out.postprocessed')\n",
    "            with open(p) as f:\n",
    "                y_pred_ = f.read().split('\\n')[:-1]\n",
    "\n",
    "            y_real, y_pred = [], []\n",
    "            for i, g in enumerate(gold):\n",
    "                if g['category'] not in unseen_domains:\n",
    "                    t = [' '.join(target['output']) for target in g['targets']]\n",
    "                    y_real.append(t)\n",
    "                    y_pred.append(y_pred_[i].strip())\n",
    "\n",
    "            num, dem = 0.0, 0\n",
    "            for i, y_ in enumerate(y_pred):\n",
    "                y = y_real[i]\n",
    "                if y_.strip() in y:\n",
    "                    num += 1\n",
    "                dem += 1\n",
    "            print('Task: ', task)\n",
    "            print('Set: ', _set)\n",
    "            print('Model: ', model)\n",
    "            print('Accuracy: ', round(num/dem, 2))\n",
    "            print(10 * '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseen domains:\n",
      "Task:  ordering\n",
      "Set:  dev\n",
      "Model:  random\n",
      "Accuracy:  0\n",
      "----------\n",
      "Task:  ordering\n",
      "Set:  dev\n",
      "Model:  major\n",
      "Accuracy:  0\n",
      "----------\n",
      "Task:  ordering\n",
      "Set:  dev\n",
      "Model:  transformer\n",
      "Accuracy:  0\n",
      "----------\n",
      "Task:  ordering\n",
      "Set:  dev\n",
      "Model:  rnn\n",
      "Accuracy:  0\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  dev\n",
      "Model:  random\n",
      "Accuracy:  0\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  dev\n",
      "Model:  major\n",
      "Accuracy:  0\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  dev\n",
      "Model:  transformer\n",
      "Accuracy:  0\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  dev\n",
      "Model:  rnn\n",
      "Accuracy:  0\n",
      "----------\n",
      "Task:  ordering\n",
      "Set:  test\n",
      "Model:  random\n",
      "Accuracy:  0.35\n",
      "----------\n",
      "Task:  ordering\n",
      "Set:  test\n",
      "Model:  major\n",
      "Accuracy:  0.44\n",
      "----------\n",
      "Task:  ordering\n",
      "Set:  test\n",
      "Model:  transformer\n",
      "Accuracy:  0.09\n",
      "----------\n",
      "Task:  ordering\n",
      "Set:  test\n",
      "Model:  rnn\n",
      "Accuracy:  0.1\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  test\n",
      "Model:  random\n",
      "Accuracy:  0.3\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  test\n",
      "Model:  major\n",
      "Accuracy:  0.06\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  test\n",
      "Model:  transformer\n",
      "Accuracy:  0.12\n",
      "----------\n",
      "Task:  structing\n",
      "Set:  test\n",
      "Model:  rnn\n",
      "Accuracy:  0.13\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('Unseen domains:')\n",
    "for _set in ['dev', 'test']:\n",
    "    for task in ['ordering', 'structing']:\n",
    "        gold_path=os.path.join(path, 'data', task, _set + '.json')\n",
    "        gold = json.load(open(gold_path))\n",
    "\n",
    "        for model in ['random', 'major', 'transformer', 'rnn']:\n",
    "            p = os.path.join(path, 'results/steps', task, model, _set + '.out.postprocessed')\n",
    "            with open(p) as f:\n",
    "                y_pred_ = f.read().split('\\n')[:-1]\n",
    "\n",
    "            y_real, y_pred = [], []\n",
    "            for i, g in enumerate(gold):\n",
    "                if g['category'] in unseen_domains:\n",
    "                    t = [' '.join(target['output']) for target in g['targets']]\n",
    "                    y_real.append(t)\n",
    "\n",
    "                    y_pred.append(y_pred_[i].strip())\n",
    "\n",
    "            num, dem = 0.0, 0\n",
    "            for i, y_ in enumerate(y_pred):\n",
    "                y = y_real[i]\n",
    "                if y_.strip() in y:\n",
    "                    num += 1\n",
    "                dem += 1\n",
    "            print('Task: ', task)\n",
    "            print('Set: ', _set)\n",
    "            print('Model: ', model)\n",
    "            print('Accuracy: ', round(num/dem, 2) if dem > 0 else 0)\n",
    "            print(10 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Referring Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All domains:\n",
      "REG Task:\n",
      "Set:  dev\n",
      "Baseline Accuracy:  0.54\n",
      "NeuralREG Accuracy:  0.72\n",
      "----------\n",
      "REG Task:\n",
      "Set:  test\n",
      "Baseline Accuracy:  0.51\n",
      "NeuralREG Accuracy:  0.39\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('All domains:')\n",
    "for _set in ['dev', 'test']:\n",
    "    gold_path=os.path.join(path, 'data', 'reg', _set + '.json')\n",
    "    gold = json.load(open(gold_path))\n",
    "\n",
    "    p = os.path.join(path, 'results/steps/reg', _set + '.out.postprocessed')\n",
    "    with open(p) as f:\n",
    "        y_pred_ = f.read().split('\\n')[:-1]\n",
    "\n",
    "    y_real, y_pred, y_baseline = [], [], []\n",
    "    for i, g in enumerate(gold):\n",
    "        y_real.append(' '.join(g['refex']).strip().lower())\n",
    "        y_pred.append(y_pred_[i].strip().lower())\n",
    "    \n",
    "    num, dem = 0.0, 0\n",
    "    baseline = 0\n",
    "    for i, y_ in enumerate(y_pred):\n",
    "        refex = ' '.join(nltk.word_tokenize(gold[i]['entity'].replace('\\'', ' ').replace('\\\"', ' ').replace('_', ' ')))\n",
    "        y = y_real[i]\n",
    "        if y_.strip() == y:\n",
    "            num += 1\n",
    "        if refex.strip().lower() == y:\n",
    "            baseline += 1\n",
    "        dem += 1\n",
    "    print('REG Task:')\n",
    "    print('Set: ', _set)\n",
    "    print('Baseline Accuracy: ', round(baseline/dem, 2) if dem > 0 else 0)\n",
    "    print('NeuralREG Accuracy: ', round(num/dem, 2) if dem > 0 else 0)\n",
    "    print(10 * '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen domains:\n",
      "REG Task:\n",
      "Set:  dev\n",
      "Baseline Accuracy:  0.54\n",
      "NeuralREG Accuracy:  0.72\n",
      "----------\n",
      "REG Task:\n",
      "Set:  test\n",
      "Baseline Accuracy:  0.53\n",
      "NeuralREG Accuracy:  0.7\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('Seen domains:')\n",
    "for _set in ['dev', 'test']:\n",
    "    gold_path=os.path.join(path, 'data', 'reg', _set + '.json')\n",
    "    gold = json.load(open(gold_path))\n",
    "\n",
    "    p = os.path.join(path, 'results/steps/reg', _set + '.out.postprocessed')\n",
    "    with open(p) as f:\n",
    "        y_pred_ = f.read().split('\\n')[:-1]\n",
    "\n",
    "    y_real, y_pred, y_baseline = [], [], []\n",
    "    for i, g in enumerate(gold):\n",
    "    #     if g['category'] in unseen_domains:\n",
    "        y_real.append(' '.join(g['refex']).strip().lower())\n",
    "        y_pred.append(y_pred_[i].strip().lower())\n",
    "    \n",
    "    num, dem = 0.0, 0\n",
    "    baseline = 0\n",
    "    for i, y_ in enumerate(y_pred):\n",
    "        if gold[i]['category'] not in unseen_domains:\n",
    "            refex = ' '.join(nltk.word_tokenize(gold[i]['entity'].replace('\\'', ' ').replace('\\\"', ' ').replace('_', ' ')))\n",
    "            y = y_real[i]\n",
    "            if y_.strip() == y:\n",
    "                num += 1\n",
    "            if refex.strip().lower() == y:\n",
    "                baseline += 1\n",
    "            dem += 1\n",
    "    print('REG Task:')\n",
    "    print('Set: ', _set)\n",
    "    print('Baseline Accuracy: ', round(baseline/dem, 2) if dem > 0 else 0)\n",
    "    print('NeuralREG Accuracy: ', round(num/dem, 2) if dem > 0 else 0)\n",
    "    print(10 * '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseen domains:\n",
      "REG Task:\n",
      "Set:  dev\n",
      "Baseline Accuracy:  0\n",
      "NeuralREG Accuracy:  0\n",
      "----------\n",
      "REG Task:\n",
      "Set:  test\n",
      "Baseline Accuracy:  0.5\n",
      "NeuralREG Accuracy:  0.07\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('Unseen domains:')\n",
    "for _set in ['dev', 'test']:\n",
    "    gold_path=os.path.join(path, 'data', 'reg', _set + '.json')\n",
    "    gold = json.load(open(gold_path))\n",
    "\n",
    "    p = os.path.join(path, 'results/steps/reg', _set + '.out.postprocessed')\n",
    "    with open(p) as f:\n",
    "        y_pred_ = f.read().split('\\n')[:-1]\n",
    "\n",
    "    y_real, y_pred, y_baseline = [], [], []\n",
    "    for i, g in enumerate(gold):\n",
    "        y_real.append(' '.join(g['refex']).strip().lower())\n",
    "        y_pred.append(y_pred_[i].strip().lower())\n",
    "    \n",
    "    num, dem = 0.0, 0\n",
    "    baseline = 0\n",
    "    for i, y_ in enumerate(y_pred):\n",
    "        if gold[i]['category'] in unseen_domains:\n",
    "            refex = ' '.join(nltk.word_tokenize(gold[i]['entity'].replace('\\'', ' ').replace('\\\"', ' ').replace('_', ' ')))\n",
    "            y = y_real[i]\n",
    "            if y_.strip() == y:\n",
    "                num += 1\n",
    "            if refex.strip().lower() == y:\n",
    "                baseline += 1\n",
    "            dem += 1\n",
    "    print('REG Task:')\n",
    "    print('Set: ', _set)\n",
    "    print('Baseline Accuracy: ', round(baseline/dem, 2) if dem > 0 else 0)\n",
    "    print('NeuralREG Accuracy: ', round(num/dem, 2) if dem > 0 else 0)\n",
    "    print(10 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Lexicalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All domains:\n",
      "Lexicalization: \n",
      "Set:  dev\n",
      "Model:  random\n",
      "b'BLEU = 40.06, 72.4/46.1/32.0/24.1 (BP=1.000, ratio=1.085, hyp_len=22790, ref_len=21003)\\n'\n",
      "----------\n",
      "Lexicalization: \n",
      "Set:  dev\n",
      "Model:  major\n",
      "b'BLEU = 43.26, 74.8/49.1/35.3/27.0 (BP=1.000, ratio=1.071, hyp_len=22286, ref_len=20813)\\n'\n",
      "----------\n",
      "Lexicalization: \n",
      "Set:  dev\n",
      "Model:  transformer\n",
      "b'BLEU = 48.69, 76.0/54.2/41.3/33.0 (BP=1.000, ratio=1.018, hyp_len=34167, ref_len=33579)\\n'\n",
      "----------\n",
      "Lexicalization: \n",
      "Set:  dev\n",
      "Model:  rnn\n",
      "b'BLEU = 49.71, 77.0/55.1/42.3/34.1 (BP=1.000, ratio=1.006, hyp_len=33998, ref_len=33803)\\n'\n",
      "----------\n",
      "Lexicalization: \n",
      "Set:  test\n",
      "Model:  random\n",
      "b'BLEU = 39.49, 72.6/45.5/31.4/23.4 (BP=1.000, ratio=1.073, hyp_len=29090, ref_len=27122)\\n'\n",
      "----------\n",
      "Lexicalization: \n",
      "Set:  test\n",
      "Model:  major\n",
      "b'BLEU = 44.82, 76.2/50.3/36.9/28.5 (BP=1.000, ratio=1.069, hyp_len=27956, ref_len=26163)\\n'\n",
      "----------\n",
      "Lexicalization: \n",
      "Set:  test\n",
      "Model:  transformer\n",
      "b'BLEU = 38.12, 72.8/46.2/31.6/23.7 (BP=0.956, ratio=0.957, hyp_len=67641, ref_len=70653)\\n'\n",
      "----------\n",
      "Lexicalization: \n",
      "Set:  test\n",
      "Model:  rnn\n",
      "b'BLEU = 37.43, 67.7/43.5/29.8/22.4 (BP=1.000, ratio=1.038, hyp_len=75752, ref_len=73008)\\n'\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('All domains:')\n",
    "for _set in ['dev', 'test']:\n",
    "    for model in ['random', 'major', 'transformer', 'rnn']:\n",
    "        gold_path=os.path.join(path, 'data', 'lexicalization', _set + '.json')\n",
    "        gold = json.load(open(gold_path))\n",
    "\n",
    "        p = os.path.join(path, 'results/steps/lexicalization', model, _set + '.out.postprocessed')\n",
    "        with open(p) as f:\n",
    "            y_pred_ = f.read().split('\\n')[:-1]\n",
    "\n",
    "        y_real, y_pred = [], []\n",
    "        for i, g in enumerate(gold):\n",
    "        #     if g['category'] in unseen_domains:\n",
    "            t = [' '.join(target['output']).lower() for target in g['targets']]\n",
    "            y_real.append(t)\n",
    "            y_pred.append(y_pred_[i].strip().lower())\n",
    "\n",
    "        with open('predictions', 'w') as f:\n",
    "            f.write('\\n'.join(y_pred))\n",
    "\n",
    "        nfiles = max([len(refs) for refs in y_real])\n",
    "        for i in range(nfiles):\n",
    "            with open('reference' + str(i+1), 'w') as f:\n",
    "                for refs in y_real:\n",
    "                    if i < len(refs):\n",
    "                        f.write(refs[i])\n",
    "                    f.write('\\n')\n",
    "\n",
    "        nematus = '/roaming/tcastrof/workspace/nematus/data/multi-bleu.perl'\n",
    "        command = 'perl ' + nematus + ' reference1 reference2 reference3 reference4 reference5 reference6 reference7 reference8 < predictions'\n",
    "        result = subprocess.check_output(command, shell=True)\n",
    "        print('Lexicalization: ')\n",
    "        print('Set: ', _set)\n",
    "        print('Model: ', model)\n",
    "        print(result)\n",
    "        print(10 * '-')\n",
    "\n",
    "        os.remove('reference1')\n",
    "        os.remove('reference2')\n",
    "        os.remove('reference3')\n",
    "        os.remove('reference4')\n",
    "        os.remove('reference5')\n",
    "        os.remove('reference6')\n",
    "        os.remove('reference7')\n",
    "        os.remove('reference8')\n",
    "        os.remove('predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen domains:\n",
      "Lexicalization: \n",
      "Set:  dev\n",
      "Model:  random\n",
      "b'BLEU = 40.06, 72.4/46.1/32.0/24.1 (BP=1.000, ratio=1.085, hyp_len=22790, ref_len=21003)\\n'\n",
      "----------\n",
      "Lexicalization: \n",
      "Set:  dev\n",
      "Model:  major\n",
      "b'BLEU = 43.26, 74.8/49.1/35.3/27.0 (BP=1.000, ratio=1.071, hyp_len=22286, ref_len=20813)\\n'\n",
      "----------\n",
      "Lexicalization: \n",
      "Set:  dev\n",
      "Model:  transformer\n",
      "b'BLEU = 48.69, 76.0/54.2/41.3/33.0 (BP=1.000, ratio=1.018, hyp_len=34167, ref_len=33579)\\n'\n",
      "----------\n",
      "Lexicalization: \n",
      "Set:  dev\n",
      "Model:  rnn\n",
      "b'BLEU = 49.71, 77.0/55.1/42.3/34.1 (BP=1.000, ratio=1.006, hyp_len=33998, ref_len=33803)\\n'\n",
      "----------\n",
      "Lexicalization: \n",
      "Set:  test\n",
      "Model:  random\n",
      "b'BLEU = 40.46, 73.1/46.6/32.4/24.2 (BP=1.000, ratio=1.040, hyp_len=24491, ref_len=23559)\\n'\n",
      "----------\n",
      "Lexicalization: \n",
      "Set:  test\n",
      "Model:  major\n",
      "b'BLEU = 45.65, 76.6/51.2/37.8/29.3 (BP=1.000, ratio=1.033, hyp_len=23805, ref_len=23048)\\n'\n",
      "----------\n",
      "Lexicalization: \n",
      "Set:  test\n",
      "Model:  transformer\n",
      "b'BLEU = 48.14, 77.5/54.6/41.4/32.6 (BP=0.985, ratio=0.985, hyp_len=38749, ref_len=39330)\\n'\n",
      "----------\n",
      "Lexicalization: \n",
      "Set:  test\n",
      "Model:  rnn\n",
      "b'BLEU = 49.26, 77.8/55.1/42.0/33.5 (BP=0.994, ratio=0.994, hyp_len=39074, ref_len=39307)\\n'\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('Seen domains:')\n",
    "for _set in ['dev', 'test']:\n",
    "    for model in ['random', 'major', 'transformer', 'rnn']:\n",
    "        gold_path=os.path.join(path, 'data', 'lexicalization', _set + '.json')\n",
    "        gold = json.load(open(gold_path))\n",
    "\n",
    "        p = os.path.join(path, 'results/steps/lexicalization', model, _set + '.out.postprocessed')\n",
    "        with open(p) as f:\n",
    "            y_pred_ = f.read().split('\\n')[:-1]\n",
    "\n",
    "        y_real, y_pred = [], []\n",
    "        for i, g in enumerate(gold):\n",
    "            if g['category'] not in unseen_domains:\n",
    "                t = [' '.join(target['output']).lower() for target in g['targets']]\n",
    "                y_real.append(t)\n",
    "                y_pred.append(y_pred_[i].strip().lower())\n",
    "\n",
    "        with open('predictions', 'w') as f:\n",
    "            f.write('\\n'.join(y_pred))\n",
    "\n",
    "        nfiles = max([len(refs) for refs in y_real])\n",
    "        for i in range(nfiles):\n",
    "            with open('reference' + str(i+1), 'w') as f:\n",
    "                for refs in y_real:\n",
    "                    if i < len(refs):\n",
    "                        f.write(refs[i])\n",
    "                    f.write('\\n')\n",
    "\n",
    "        nematus = '/roaming/tcastrof/workspace/nematus/data/multi-bleu.perl'\n",
    "        command = 'perl ' + nematus + ' reference1 reference2 reference3 reference4 reference5 reference6 reference7 reference8 < predictions'\n",
    "        result = subprocess.check_output(command, shell=True)\n",
    "        print('Lexicalization: ')\n",
    "        print('Set: ', _set)\n",
    "        print('Model: ', model)\n",
    "        print(result)\n",
    "        print(10 * '-')\n",
    "\n",
    "        os.remove('predictions')\n",
    "        os.remove('reference1')\n",
    "        os.remove('reference2')\n",
    "        os.remove('reference3')\n",
    "        os.remove('reference4')\n",
    "        os.remove('reference5')\n",
    "        os.remove('reference6')\n",
    "        os.remove('reference7')\n",
    "        os.remove('reference8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseen domains:\n",
      "Lexicalization: \n",
      "Set:  test\n",
      "Model:  random\n",
      "b'BLEU = 33.79, 69.8/39.8/25.5/18.4 (BP=1.000, ratio=1.288, hyp_len=4599, ref_len=3572)\\n'\n",
      "----------\n",
      "Lexicalization: \n",
      "Set:  test\n",
      "Model:  major\n",
      "b'BLEU = 39.43, 73.9/44.9/31.4/23.2 (BP=1.000, ratio=1.329, hyp_len=4151, ref_len=3124)\\n'\n",
      "----------\n",
      "Lexicalization: \n",
      "Set:  test\n",
      "Model:  transformer\n",
      "b'BLEU = 24.15, 66.4/34.7/18.2/11.4 (BP=0.919, ratio=0.922, hyp_len=28892, ref_len=31323)\\n'\n",
      "----------\n",
      "Lexicalization: \n",
      "Set:  test\n",
      "Model:  rnn\n",
      "b'BLEU = 23.63, 57.0/31.1/16.8/10.5 (BP=1.000, ratio=1.088, hyp_len=36678, ref_len=33701)\\n'\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('Unseen domains:')\n",
    "for _set in ['dev', 'test']:\n",
    "    for model in ['random', 'major', 'transformer', 'rnn']:\n",
    "        gold_path=os.path.join(path, 'data', 'lexicalization', _set + '.json')\n",
    "        gold = json.load(open(gold_path))\n",
    "\n",
    "        p = os.path.join(path, 'results/steps/lexicalization', model, _set + '.out.postprocessed')\n",
    "        with open(p) as f:\n",
    "            y_pred_ = f.read().split('\\n')[:-1]\n",
    "\n",
    "        y_real, y_pred = [], []\n",
    "        for i, g in enumerate(gold):\n",
    "            if g['category'] in unseen_domains:\n",
    "                t = [' '.join(target['output']).lower() for target in g['targets']]\n",
    "                y_real.append(t)\n",
    "                y_pred.append(y_pred_[i].strip().lower())\n",
    "\n",
    "        with open('predictions', 'w') as f:\n",
    "            f.write('\\n'.join(y_pred))\n",
    "\n",
    "        try:\n",
    "            nfiles = max([len(refs) for refs in y_real])\n",
    "            for i in range(nfiles):\n",
    "                with open('reference' + str(i+1), 'w') as f:\n",
    "                    for refs in y_real:\n",
    "                        if i < len(refs):\n",
    "                            f.write(refs[i])\n",
    "                        f.write('\\n')\n",
    "\n",
    "            nematus = '/roaming/tcastrof/workspace/nematus/data/multi-bleu.perl'\n",
    "            command = 'perl ' + nematus + ' reference1 reference2 reference3 reference4 reference5 reference6 reference7 reference8 < predictions'\n",
    "            result = subprocess.check_output(command, shell=True)\n",
    "            print('Lexicalization: ')\n",
    "            print('Set: ', _set)\n",
    "            print('Model: ', model)\n",
    "            print(result)\n",
    "            print(10 * '-')\n",
    "\n",
    "            os.remove('predictions')\n",
    "            os.remove('reference1')\n",
    "            os.remove('reference2')\n",
    "            os.remove('reference3')\n",
    "            os.remove('reference4')\n",
    "            os.remove('reference5')\n",
    "            os.remove('reference6')\n",
    "            os.remove('reference7')\n",
    "            os.remove('reference8')\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Final Texts (BLEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All domains:\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: pipeline\n",
      "Model:  rand\n",
      "b'BLEU = 41.01, 75.2/50.1/33.5/22.4 (BP=1.000, ratio=1.132, hyp_len=13392, ref_len=11826)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: pipeline\n",
      "Model:  major\n",
      "b'BLEU = 46.76, 79.1/55.9/39.3/27.5 (BP=1.000, ratio=1.039, hyp_len=20297, ref_len=19539)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: pipeline\n",
      "Model:  transformer\n",
      "b'BLEU = 57.85, 87.5/67.2/51.2/39.2 (BP=0.987, ratio=0.987, hyp_len=18855, ref_len=19109)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: pipeline\n",
      "Model:  rnn\n",
      "b'BLEU = 58.69, 87.1/67.1/51.6/39.8 (BP=0.997, ratio=0.997, hyp_len=19403, ref_len=19464)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: end2end\n",
      "Model:  transformer\n",
      "b'BLEU = 55.02, 82.7/62.4/47.8/37.1 (BP=1.000, ratio=1.008, hyp_len=19494, ref_len=19343)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: end2end\n",
      "Model:  rnn\n",
      "b'BLEU = 60.19, 85.4/67.6/53.5/42.6 (BP=1.000, ratio=1.000, hyp_len=19389, ref_len=19398)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  rand\n",
      "b'BLEU = 41.68, 76.8/51.1/34.0/22.6 (BP=1.000, ratio=1.133, hyp_len=19903, ref_len=17563)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  major\n",
      "b'BLEU = 43.82, 77.1/52.9/36.3/24.9 (BP=1.000, ratio=1.071, hyp_len=33747, ref_len=31517)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  transformer\n",
      "b'BLEU = 51.68, 83.0/59.5/44.0/32.8 (BP=1.000, ratio=1.026, hyp_len=30942, ref_len=30160)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  rnn\n",
      "b'BLEU = 50.55, 81.3/58.3/43.0/32.1 (BP=1.000, ratio=1.026, hyp_len=32557, ref_len=31718)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: end2end\n",
      "Model:  transformer\n",
      "b'BLEU = 31.88, 61.1/37.1/26.2/19.1 (BP=0.977, ratio=0.977, hyp_len=38993, ref_len=39898)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: end2end\n",
      "Model:  rnn\n",
      "b'BLEU = 33.49, 58.1/37.3/27.5/21.1 (BP=1.000, ratio=1.042, hyp_len=42722, ref_len=41009)\\n'\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('All domains:')\n",
    "for _set in ['dev', 'test']:\n",
    "    gold_path=os.path.join(path, 'data', 'end2end', _set + '.json')\n",
    "    gold = json.load(open(gold_path))\n",
    "    for kind in ['pipeline', 'end2end']:\n",
    "        for model in ['rand', 'major', 'transformer', 'rnn']:\n",
    "            if kind == 'end2end' and model in ['rand', 'major']:\n",
    "                continue\n",
    "            else:\n",
    "                p = os.path.join(path, 'results', kind, model, _set + '.out.postprocessed')\n",
    "                with open(p) as f:\n",
    "                    y_pred_ = f.read().split('\\n')[:-1]\n",
    "\n",
    "                y_real, y_pred = [], []\n",
    "                for i, g in enumerate(gold):\n",
    "                #     if g['category'] in unseen_domains:\n",
    "                    targets = [nltk.word_tokenize(' '.join(target['output'])) for target in g['targets']]\n",
    "                    t = [' '.join(target).lower() for target in targets]\n",
    "                    y_real.append(t)\n",
    "                    pred = ' '.join(nltk.word_tokenize(y_pred_[i])).lower()\n",
    "                    y_pred.append(pred)\n",
    "\n",
    "                with open('predictions', 'w') as f:\n",
    "                    f.write('\\n'.join(y_pred))\n",
    "\n",
    "                nfiles = max([len(refs) for refs in y_real])\n",
    "                for i in range(nfiles):\n",
    "                    with open('reference' + str(i+1), 'w') as f:\n",
    "                        for refs in y_real:\n",
    "                            if i < len(refs):\n",
    "                                f.write(refs[i])\n",
    "                            f.write('\\n')\n",
    "\n",
    "                nematus = '/roaming/tcastrof/workspace/nematus/data/multi-bleu.perl'\n",
    "                command = 'perl ' + nematus + ' reference1 reference2 reference3 reference4 reference5 reference6 reference7 reference8 < predictions'\n",
    "                result = subprocess.check_output(command, shell=True)\n",
    "                print('Final: ')\n",
    "                print('Set: ', _set)\n",
    "                print('Approach:', kind)\n",
    "                print('Model: ', model)\n",
    "                print(result)\n",
    "                print(10 * '-')\n",
    "\n",
    "                os.remove('reference1')\n",
    "                os.remove('reference2')\n",
    "                os.remove('reference3')\n",
    "                os.remove('reference4')\n",
    "                os.remove('reference5')\n",
    "                os.remove('reference6')\n",
    "                os.remove('reference7')\n",
    "                os.remove('reference8')\n",
    "                os.remove('predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen domains:\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: pipeline\n",
      "Model:  rand\n",
      "b'BLEU = 41.01, 75.2/50.1/33.5/22.4 (BP=1.000, ratio=1.132, hyp_len=13392, ref_len=11826)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: pipeline\n",
      "Model:  major\n",
      "b'BLEU = 46.76, 79.1/55.9/39.3/27.5 (BP=1.000, ratio=1.039, hyp_len=20297, ref_len=19539)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: pipeline\n",
      "Model:  transformer\n",
      "b'BLEU = 57.85, 87.5/67.2/51.2/39.2 (BP=0.987, ratio=0.987, hyp_len=18855, ref_len=19109)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: pipeline\n",
      "Model:  rnn\n",
      "b'BLEU = 58.69, 87.1/67.1/51.6/39.8 (BP=0.997, ratio=0.997, hyp_len=19403, ref_len=19464)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: end2end\n",
      "Model:  transformer\n",
      "b'BLEU = 55.02, 82.7/62.4/47.8/37.1 (BP=1.000, ratio=1.008, hyp_len=19494, ref_len=19343)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: end2end\n",
      "Model:  rnn\n",
      "b'BLEU = 60.19, 85.4/67.6/53.5/42.6 (BP=1.000, ratio=1.000, hyp_len=19389, ref_len=19398)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  rand\n",
      "b'BLEU = 41.72, 76.3/50.9/34.0/22.9 (BP=1.000, ratio=1.118, hyp_len=15183, ref_len=13580)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  major\n",
      "b'BLEU = 44.79, 77.0/53.5/37.3/26.2 (BP=1.000, ratio=1.068, hyp_len=24117, ref_len=22572)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  transformer\n",
      "b'BLEU = 56.35, 86.7/65.5/49.4/37.6 (BP=0.989, ratio=0.989, hyp_len=20985, ref_len=21220)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  rnn\n",
      "b'BLEU = 55.75, 86.1/64.6/48.5/36.8 (BP=0.993, ratio=0.993, hyp_len=21523, ref_len=21667)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: end2end\n",
      "Model:  transformer\n",
      "b'BLEU = 50.79, 80.6/58.7/43.6/32.8 (BP=0.996, ratio=0.996, hyp_len=21854, ref_len=21941)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: end2end\n",
      "Model:  rnn\n",
      "b'BLEU = 57.20, 83.8/64.8/50.5/39.9 (BP=0.994, ratio=0.994, hyp_len=21815, ref_len=21943)\\n'\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('Seen domains:')\n",
    "for _set in ['dev', 'test']:\n",
    "    gold_path=os.path.join(path, 'data', 'end2end', _set + '.json')\n",
    "    gold = json.load(open(gold_path))\n",
    "    for kind in ['pipeline', 'end2end']:\n",
    "        for model in ['rand', 'major', 'transformer', 'rnn']:\n",
    "            if kind == 'end2end' and model in ['rand', 'major']:\n",
    "                continue\n",
    "            else:\n",
    "                p = os.path.join(path, 'results', kind, model, _set + '.out.postprocessed')\n",
    "                with open(p) as f:\n",
    "                    y_pred_ = f.read().split('\\n')[:-1]\n",
    "\n",
    "                y_real, y_pred = [], []\n",
    "                for i, g in enumerate(gold):\n",
    "                    if g['category'] not in unseen_domains:\n",
    "                        targets = [nltk.word_tokenize(' '.join(target['output'])) for target in g['targets']]\n",
    "                        t = [' '.join(target).lower() for target in targets]\n",
    "                        y_real.append(t)\n",
    "                        pred = ' '.join(nltk.word_tokenize(y_pred_[i])).lower()\n",
    "                        y_pred.append(pred)\n",
    "\n",
    "                with open('predictions', 'w') as f:\n",
    "                    f.write('\\n'.join(y_pred))\n",
    "\n",
    "                nfiles = max([len(refs) for refs in y_real])\n",
    "                for i in range(nfiles):\n",
    "                    with open('reference' + str(i+1), 'w') as f:\n",
    "                        for refs in y_real:\n",
    "                            if i < len(refs):\n",
    "                                f.write(refs[i])\n",
    "                            f.write('\\n')\n",
    "\n",
    "                nematus = '/roaming/tcastrof/workspace/nematus/data/multi-bleu.perl'\n",
    "                command = 'perl ' + nematus + ' reference1 reference2 reference3 reference4 reference5 reference6 reference7 reference8 < predictions'\n",
    "                result = subprocess.check_output(command, shell=True)\n",
    "                print('Final: ')\n",
    "                print('Set: ', _set)\n",
    "                print('Approach:', kind)\n",
    "                print('Model: ', model)\n",
    "                print(result)\n",
    "                print(10 * '-')\n",
    "\n",
    "                os.remove('reference1')\n",
    "                os.remove('reference2')\n",
    "                os.remove('reference3')\n",
    "                os.remove('reference4')\n",
    "                os.remove('reference5')\n",
    "                os.remove('reference6')\n",
    "                os.remove('reference7')\n",
    "                os.remove('reference8')\n",
    "                os.remove('predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseen domains:\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  rand\n",
      "b'BLEU = 41.51, 78.6/51.6/33.9/21.6 (BP=1.000, ratio=1.182, hyp_len=4720, ref_len=3994)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  major\n",
      "b'BLEU = 41.13, 77.3/51.4/33.7/21.4 (BP=1.000, ratio=1.075, hyp_len=9630, ref_len=8956)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  transformer\n",
      "b'BLEU = 38.92, 75.3/46.3/31.5/20.9 (BP=1.000, ratio=1.114, hyp_len=9957, ref_len=8940)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  rnn\n",
      "b'BLEU = 38.55, 71.9/45.5/31.3/21.6 (BP=1.000, ratio=1.098, hyp_len=11034, ref_len=10051)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: end2end\n",
      "Model:  transformer\n",
      "b'BLEU = 5.88, 36.3/9.3/3.6/1.2 (BP=0.953, ratio=0.954, hyp_len=17139, ref_len=17957)\\n'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: end2end\n",
      "Model:  rnn\n",
      "b'BLEU = 6.24, 31.3/8.7/3.6/1.5 (BP=1.000, ratio=1.097, hyp_len=20907, ref_len=19066)\\n'\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('Unseen domains:')\n",
    "for _set in ['test']:\n",
    "    gold_path=os.path.join(path, 'data', 'end2end', _set + '.json')\n",
    "    gold = json.load(open(gold_path))\n",
    "    for kind in ['pipeline', 'end2end']:\n",
    "        for model in ['rand', 'major', 'transformer', 'rnn']:\n",
    "            if kind == 'end2end' and model in ['rand', 'major']:\n",
    "                continue\n",
    "            else:\n",
    "                p = os.path.join(path, 'results', kind, model, _set + '.out.postprocessed')\n",
    "                with open(p) as f:\n",
    "                    y_pred_ = f.read().split('\\n')[:-1]\n",
    "\n",
    "                y_real, y_pred = [], []\n",
    "                for i, g in enumerate(gold):\n",
    "                    if g['category'] in unseen_domains:\n",
    "                        targets = [nltk.word_tokenize(' '.join(target['output'])) for target in g['targets']]\n",
    "                        t = [' '.join(target).lower() for target in targets]\n",
    "                        y_real.append(t)\n",
    "                        pred = ' '.join(nltk.word_tokenize(y_pred_[i])).lower()\n",
    "                        y_pred.append(pred)\n",
    "\n",
    "                with open('predictions', 'w') as f:\n",
    "                    f.write('\\n'.join(y_pred))\n",
    "\n",
    "                nfiles = max([len(refs) for refs in y_real])\n",
    "                for i in range(nfiles):\n",
    "                    with open('reference' + str(i+1), 'w') as f:\n",
    "                        for refs in y_real:\n",
    "                            if i < len(refs):\n",
    "                                f.write(refs[i])\n",
    "                            f.write('\\n')\n",
    "\n",
    "                nematus = '/roaming/tcastrof/workspace/nematus/data/multi-bleu.perl'\n",
    "                command = 'perl ' + nematus + ' reference1 reference2 reference3 reference4 reference5 reference6 reference7 reference8 < predictions'\n",
    "                result = subprocess.check_output(command, shell=True)\n",
    "                print('Final: ')\n",
    "                print('Set: ', _set)\n",
    "                print('Approach:', kind)\n",
    "                print('Model: ', model)\n",
    "                print(result)\n",
    "                print(10 * '-')\n",
    "\n",
    "                try:\n",
    "                    os.remove('predictions')\n",
    "                    os.remove('reference1')\n",
    "                    os.remove('reference2')\n",
    "                    os.remove('reference3')\n",
    "                    os.remove('reference4')\n",
    "                    os.remove('reference5')\n",
    "                    os.remove('reference6')\n",
    "                    os.remove('reference7')\n",
    "                    os.remove('reference8')\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Final Texts (METEOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All domains:\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: pipeline\n",
      "Model:  rand\n",
      "b''\n",
      "----------\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: pipeline\n",
      "Model:  major\n",
      "b'Final score:            0.41077562572144327'\n",
      "----------\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: pipeline\n",
      "Model:  transformer\n",
      "b'Final score:            0.4330416533942173'\n",
      "----------\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: pipeline\n",
      "Model:  rnn\n",
      "b'Final score:            0.44177425982219903'\n",
      "----------\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: end2end\n",
      "Model:  transformer\n",
      "b'Final score:            0.4134470330967474'\n",
      "----------\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: end2end\n",
      "Model:  rnn\n",
      "b'Final score:            0.4324570419023144'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  rand\n",
      "b'Final score:            0.20375595787202236'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  major\n",
      "b'Final score:            0.3279519530149407'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  transformer\n",
      "b'Final score:            0.3230984032371677'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  rnn\n",
      "b'Final score:            0.3286943151571517'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: end2end\n",
      "Model:  transformer\n",
      "b'Final score:            0.24515982122374186'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: end2end\n",
      "Model:  rnn\n",
      "b'Final score:            0.254004927216645'\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('All domains:')\n",
    "for _set in ['dev', 'test']:\n",
    "    gold_path=os.path.join(path, 'data', 'end2end', _set + '.json')\n",
    "    gold = json.load(open(gold_path))\n",
    "    for kind in ['pipeline', 'end2end']:\n",
    "        for model in ['rand', 'major', 'transformer', 'rnn']:\n",
    "            if kind == 'end2end' and model in ['rand', 'major']:\n",
    "                continue\n",
    "            else:\n",
    "                p = os.path.join(path, 'results', kind, model, _set + '.out.postprocessed')\n",
    "                with open(p) as f:\n",
    "                    y_pred_ = f.read().split('\\n')[:-1]\n",
    "                    \n",
    "                y_real, y_pred = [], []\n",
    "                for i, g in enumerate(gold):\n",
    "#                     if g['category'] in unseen_domains:\n",
    "                    targets = [nltk.word_tokenize(' '.join(target['output'])) for target in g['targets']]\n",
    "                    t = [' '.join(target).lower() for target in targets]\n",
    "                    y_real.append(t)\n",
    "                    pred = ' '.join(nltk.word_tokenize(y_pred_[i])).lower()\n",
    "                    y_pred.append(pred)\n",
    "\n",
    "\n",
    "                with open('predictions', 'w') as f:\n",
    "                    f.write('\\n'.join(y_pred))\n",
    "\n",
    "                with open('reference', 'w') as f:\n",
    "                    for refs in y_real:\n",
    "                        for i in range(8):\n",
    "                            if i < len(refs):\n",
    "                                f.write(refs[i])\n",
    "                            else:\n",
    "                                f.write('')\n",
    "                            f.write('\\n')\n",
    "\n",
    "                java = '/roaming/tcastrof/workspace/java/jre1.8.0_181/bin/java -Xmx2G -jar '\n",
    "                java += '/home/tcastrof/workspace/meteor-1.5/meteor-1.5.jar predictions reference -l en -norm -r 8'\n",
    "                result = subprocess.check_output(java, shell=True)\n",
    "                print('Final: ')\n",
    "                print('Set: ', _set)\n",
    "                print('Approach:', kind)\n",
    "                print('Model: ', model)\n",
    "                print(result.split(b'\\n')[-2])\n",
    "                print(10 * '-')\n",
    "\n",
    "                os.remove('reference')\n",
    "                os.remove('predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen domains:\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: pipeline\n",
      "Model:  rand\n",
      "b''\n",
      "----------\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: pipeline\n",
      "Model:  major\n",
      "b'Final score:            0.41077562572144327'\n",
      "----------\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: pipeline\n",
      "Model:  transformer\n",
      "b'Final score:            0.4330416533942173'\n",
      "----------\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: pipeline\n",
      "Model:  rnn\n",
      "b'Final score:            0.44177425982219903'\n",
      "----------\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: end2end\n",
      "Model:  transformer\n",
      "b'Final score:            0.4134470330967474'\n",
      "----------\n",
      "Final: \n",
      "Set:  dev\n",
      "Approach: end2end\n",
      "Model:  rnn\n",
      "b'Final score:            0.4324570419023144'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  rand\n",
      "b'Final score:            0.27316371692277963'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  major\n",
      "b'Final score:            0.4122845738480068'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  transformer\n",
      "b'Final score:            0.4144337367285661'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  rnn\n",
      "b'Final score:            0.42014805234215746'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: end2end\n",
      "Model:  transformer\n",
      "b'Final score:            0.38962397643540064'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: end2end\n",
      "Model:  rnn\n",
      "b'Final score:            0.41208938673817913'\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('Seen domains:')\n",
    "for _set in ['dev', 'test']:\n",
    "    gold_path=os.path.join(path, 'data', 'end2end', _set + '.json')\n",
    "    gold = json.load(open(gold_path))\n",
    "    for kind in ['pipeline', 'end2end']:\n",
    "        for model in ['rand', 'major', 'transformer', 'rnn']:\n",
    "            if kind == 'end2end' and model in ['rand', 'major']:\n",
    "                continue\n",
    "            else:\n",
    "                p = os.path.join(path, 'results', kind, model, _set + '.out.postprocessed')\n",
    "                with open(p) as f:\n",
    "                    y_pred_ = f.read().split('\\n')[:-1]\n",
    "                    \n",
    "                y_real, y_pred = [], []\n",
    "                for i, g in enumerate(gold):\n",
    "                    if g['category'] not in unseen_domains:\n",
    "                        targets = [nltk.word_tokenize(' '.join(target['output'])) for target in g['targets']]\n",
    "                        t = [' '.join(target).lower() for target in targets]\n",
    "                        y_real.append(t)\n",
    "                        pred = ' '.join(nltk.word_tokenize(y_pred_[i])).lower()\n",
    "                        y_pred.append(pred)\n",
    "\n",
    "\n",
    "                with open('predictions', 'w') as f:\n",
    "                    f.write('\\n'.join(y_pred))\n",
    "\n",
    "                with open('reference', 'w') as f:\n",
    "                    for refs in y_real:\n",
    "                        for i in range(8):\n",
    "                            if i < len(refs):\n",
    "                                f.write(refs[i])\n",
    "                            else:\n",
    "                                f.write('')\n",
    "                            f.write('\\n')\n",
    "\n",
    "                java = '/roaming/tcastrof/workspace/java/jre1.8.0_181/bin/java -Xmx2G -jar '\n",
    "                java += '/home/tcastrof/workspace/meteor-1.5/meteor-1.5.jar predictions reference -l en -norm -r 8'\n",
    "                result = subprocess.check_output(java, shell=True)\n",
    "                print('Final: ')\n",
    "                print('Set: ', _set)\n",
    "                print('Approach:', kind)\n",
    "                print('Model: ', model)\n",
    "                print(result.split(b'\\n')[-2])\n",
    "                print(10 * '-')\n",
    "\n",
    "                os.remove('reference')\n",
    "                os.remove('predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseen domains:\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  rand\n",
      "b''\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  major\n",
      "b'Final score:            0.2193301666864556'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  transformer\n",
      "b'Final score:            0.20948928354092697'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: pipeline\n",
      "Model:  rnn\n",
      "b'Final score:            0.21778297038185643'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: end2end\n",
      "Model:  transformer\n",
      "b'Final score:            0.08846673176674488'\n",
      "----------\n",
      "Final: \n",
      "Set:  test\n",
      "Approach: end2end\n",
      "Model:  rnn\n",
      "b'Final score:            0.08872436586936241'\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('Unseen domains:')\n",
    "for _set in ['test']:\n",
    "    gold_path=os.path.join(path, 'data', 'end2end', _set + '.json')\n",
    "    gold = json.load(open(gold_path))\n",
    "    for kind in ['pipeline', 'end2end']:\n",
    "        for model in ['rand', 'major', 'transformer', 'rnn']:\n",
    "            if kind == 'end2end' and model in ['rand', 'major']:\n",
    "                continue\n",
    "            else:\n",
    "                p = os.path.join(path, 'results', kind, model, _set + '.out.postprocessed')\n",
    "                with open(p) as f:\n",
    "                    y_pred_ = f.read().split('\\n')[:-1]\n",
    "                    \n",
    "                y_real, y_pred = [], []\n",
    "                for i, g in enumerate(gold):\n",
    "                    if g['category'] in unseen_domains:\n",
    "                        targets = [nltk.word_tokenize(' '.join(target['output'])) for target in g['targets']]\n",
    "                        t = [' '.join(target).lower() for target in targets]\n",
    "                        y_real.append(t)\n",
    "                        pred = ' '.join(nltk.word_tokenize(y_pred_[i])).lower()\n",
    "                        y_pred.append(pred)\n",
    "\n",
    "\n",
    "                with open('predictions', 'w') as f:\n",
    "                    f.write('\\n'.join(y_pred))\n",
    "\n",
    "                with open('reference', 'w') as f:\n",
    "                    for refs in y_real:\n",
    "                        for i in range(8):\n",
    "                            if i < len(refs):\n",
    "                                f.write(refs[i])\n",
    "                            else:\n",
    "                                f.write('')\n",
    "                            f.write('\\n')\n",
    "\n",
    "                java = '/roaming/tcastrof/workspace/java/jre1.8.0_181/bin/java -Xmx2G -jar '\n",
    "                java += '/home/tcastrof/workspace/meteor-1.5/meteor-1.5.jar predictions reference -l en -norm -r 8'\n",
    "                result = subprocess.check_output(java, shell=True)\n",
    "                print('Final: ')\n",
    "                print('Set: ', _set)\n",
    "                print('Approach:', kind)\n",
    "                print('Model: ', model)\n",
    "                print(result.split(b'\\n')[-2])\n",
    "                print(10 * '-')\n",
    "\n",
    "                os.remove('reference')\n",
    "                os.remove('predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Final Texts (Fluency and Semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, h, m-h, m+h\n",
    "\n",
    "path = 'evaluation/human/grades.json'\n",
    "grades = json.load(open(path))\n",
    "\n",
    "path = 'evaluation/human/participants.json'\n",
    "participants = json.load(open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants:  35\n",
      "***Gender:***\n",
      "Counter({'M': 21, 'F': 14})\n",
      "***English Proficiency Level:***\n",
      "Counter({'native': 18, 'fluent': 17})\n",
      "***Age:***\n",
      "32.29\n",
      "\n",
      "\n",
      "All Domains\n",
      "Fluency: \n",
      "rand: 4.55 +-0.27\n",
      "major: 5.0 +-0.23\n",
      "rnn: 5.31 +-0.23\n",
      "transformer: 5.03 +-0.25\n",
      "e2ernn: 4.73 +-0.24\n",
      "e2etransformer: 5.02 +-0.25\n",
      "melbourne: 5.04 +-0.22\n",
      "upfforge: 5.46 +-0.19\n",
      "original: 5.76 +-0.17\n",
      "\n",
      "\n",
      "Semantics: \n",
      "rand: 4.44 +-0.29\n",
      "major: 5.02 +-0.24\n",
      "rnn: 5.21 +-0.23\n",
      "transformer: 4.87 +-0.27\n",
      "e2ernn: 4.47 +-0.26\n",
      "e2etransformer: 4.7 +-0.27\n",
      "melbourne: 4.94 +-0.24\n",
      "upfforge: 5.31 +-0.21\n",
      "original: 5.74 +-0.18\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "grades = json.load(open('evaluation/human/ngrades.json'))\n",
    "participant_ids = set([w['participant_id'] for w in grades])\n",
    "print('Number of participants: ', len(participant_ids))\n",
    "\n",
    "participants = [p for p in participants if p['id'] in participant_ids]\n",
    "\n",
    "print('***Gender:***')\n",
    "print(Counter([p['gender'] for p in participants]))\n",
    "print('***English Proficiency Level:***')\n",
    "print(Counter([p['english_proficiency_level'] for p in participants]))\n",
    "print('***Age:***')\n",
    "print(round(np.mean([int(p['age']) for p in participants]), 2))\n",
    "print('\\n')\n",
    "    \n",
    "print('All Domains')\n",
    "models = set([g['model'] for g in grades])\n",
    "print('Fluency: ')\n",
    "for model in ['rand', 'major', 'rnn', 'transformer', 'e2ernn', 'e2etransformer', 'melbourne', 'upfforge', 'original']:\n",
    "    fluency = [float(g['fluency']) for g in grades if g['model'] == model]# and g['category'] in unseen_domains]\n",
    "    print('{0}: {1} +-{2}'.format(model, round(np.mean(fluency), 2), round(mean_confidence_interval(fluency)[1], 2)))\n",
    "print('\\n')\n",
    "print('Semantics: ')\n",
    "for model in ['rand', 'major', 'rnn', 'transformer', 'e2ernn', 'e2etransformer', 'melbourne', 'upfforge', 'original']:\n",
    "    semantic = [float(g['semantic']) for g in grades if g['model'] == model]# and g['category'] in unseen_domains]\n",
    "    print('{0}: {1} +-{2}'.format(model, round(np.mean(semantic), 2), round(mean_confidence_interval(semantic)[1], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen Domains\n",
      "Fluency: \n",
      "rand: 4.79 +-0.31\n",
      "major: 5.25 +-0.24\n",
      "rnn: 5.51 +-0.25\n",
      "transformer: 5.53 +-0.24\n",
      "e2ernn: 5.4 +-0.23\n",
      "e2etransformer: 5.38 +-0.26\n",
      "melbourne: 5.23 +-0.27\n",
      "upfforge: 5.43 +-0.22\n",
      "original: 5.82 +-0.2\n",
      "\n",
      "\n",
      "Semantics: \n",
      "rand: 4.73 +-0.34\n",
      "major: 5.41 +-0.24\n",
      "rnn: 5.48 +-0.25\n",
      "transformer: 5.49 +-0.27\n",
      "e2ernn: 5.21 +-0.25\n",
      "e2etransformer: 5.15 +-0.29\n",
      "melbourne: 5.33 +-0.28\n",
      "upfforge: 5.35 +-0.25\n",
      "original: 5.8 +-0.2\n"
     ]
    }
   ],
   "source": [
    "print('Seen Domains')\n",
    "models = set([g['model'] for g in grades])\n",
    "print('Fluency: ')\n",
    "for model in ['rand', 'major', 'rnn', 'transformer', 'e2ernn', 'e2etransformer', 'melbourne', 'upfforge', 'original']:\n",
    "    fluency = [float(g['fluency']) for g in grades if g['model'] == model and g['category'] not in unseen_domains]\n",
    "    print('{0}: {1} +-{2}'.format(model, round(np.mean(fluency), 2), round(mean_confidence_interval(fluency)[1], 2)))\n",
    "print('\\n')\n",
    "print('Semantics: ')\n",
    "for model in ['rand', 'major', 'rnn', 'transformer', 'e2ernn', 'e2etransformer', 'melbourne', 'upfforge', 'original']:\n",
    "    semantic = [float(g['semantic']) for g in grades if g['model'] == model and g['category'] not in unseen_domains]\n",
    "    print('{0}: {1} +-{2}'.format(model, round(np.mean(semantic), 2), round(mean_confidence_interval(semantic)[1], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseen Domains\n",
      "Fluency: \n",
      "rand: 4.07 +-0.53\n",
      "major: 4.49 +-0.48\n",
      "rnn: 4.91 +-0.48\n",
      "transformer: 4.05 +-0.5\n",
      "e2ernn: 3.45 +-0.45\n",
      "e2etransformer: 4.32 +-0.5\n",
      "melbourne: 4.65 +-0.39\n",
      "upfforge: 5.51 +-0.35\n",
      "original: 5.63 +-0.32\n",
      "\n",
      "\n",
      "Semantics: \n",
      "rand: 3.86 +-0.54\n",
      "major: 4.25 +-0.49\n",
      "rnn: 4.67 +-0.47\n",
      "transformer: 3.64 +-0.49\n",
      "e2ernn: 3.03 +-0.44\n",
      "e2etransformer: 3.81 +-0.54\n",
      "melbourne: 4.15 +-0.43\n",
      "upfforge: 5.24 +-0.4\n",
      "original: 5.63 +-0.34\n"
     ]
    }
   ],
   "source": [
    "print('Unseen Domains')\n",
    "models = set([g['model'] for g in grades])\n",
    "print('Fluency: ')\n",
    "for model in ['rand', 'major', 'rnn', 'transformer', 'e2ernn', 'e2etransformer', 'melbourne', 'upfforge', 'original']:\n",
    "    fluency = [float(g['fluency']) for g in grades if g['model'] == model and g['category'] in unseen_domains]\n",
    "    print('{0}: {1} +-{2}'.format(model, round(np.mean(fluency), 2), round(mean_confidence_interval(fluency)[1], 2)))\n",
    "print('\\n')\n",
    "print('Semantics: ')\n",
    "for model in ['rand', 'major', 'rnn', 'transformer', 'e2ernn', 'e2etransformer', 'melbourne', 'upfforge', 'original']:\n",
    "    semantic = [float(g['semantic']) for g in grades if g['model'] == model and g['category'] in unseen_domains]\n",
    "    print('{0}: {1} +-{2}'.format(model, round(np.mean(semantic), 2), round(mean_confidence_interval(semantic)[1], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand x major : True\n",
      "rand x rnn : True\n",
      "rand x transformer : True\n",
      "rand x e2ernn : False\n",
      "rand x e2etransformer : False\n",
      "rand x melbourne : True\n",
      "rand x upfforge : True\n",
      "rand x original : True\n",
      "major x rand : True\n",
      "major x rnn : False\n",
      "major x transformer : False\n",
      "major x e2ernn : True\n",
      "major x e2etransformer : False\n",
      "major x melbourne : False\n",
      "major x upfforge : False\n",
      "major x original : True\n",
      "rnn x rand : True\n",
      "rnn x major : False\n",
      "rnn x transformer : False\n",
      "rnn x e2ernn : True\n",
      "rnn x e2etransformer : True\n",
      "rnn x melbourne : False\n",
      "rnn x upfforge : False\n",
      "rnn x original : True\n",
      "transformer x rand : True\n",
      "transformer x major : False\n",
      "transformer x rnn : False\n",
      "transformer x e2ernn : True\n",
      "transformer x e2etransformer : False\n",
      "transformer x melbourne : False\n",
      "transformer x upfforge : True\n",
      "transformer x original : True\n",
      "e2ernn x rand : False\n",
      "e2ernn x major : True\n",
      "e2ernn x rnn : True\n",
      "e2ernn x transformer : True\n",
      "e2ernn x e2etransformer : False\n",
      "e2ernn x melbourne : True\n",
      "e2ernn x upfforge : True\n",
      "e2ernn x original : True\n",
      "e2etransformer x rand : False\n",
      "e2etransformer x major : False\n",
      "e2etransformer x rnn : True\n",
      "e2etransformer x transformer : False\n",
      "e2etransformer x e2ernn : False\n",
      "e2etransformer x melbourne : False\n",
      "e2etransformer x upfforge : True\n",
      "e2etransformer x original : True\n",
      "melbourne x rand : True\n",
      "melbourne x major : False\n",
      "melbourne x rnn : False\n",
      "melbourne x transformer : False\n",
      "melbourne x e2ernn : True\n",
      "melbourne x e2etransformer : False\n",
      "melbourne x upfforge : True\n",
      "melbourne x original : True\n",
      "upfforge x rand : True\n",
      "upfforge x major : False\n",
      "upfforge x rnn : False\n",
      "upfforge x transformer : True\n",
      "upfforge x e2ernn : True\n",
      "upfforge x e2etransformer : True\n",
      "upfforge x melbourne : True\n",
      "upfforge x original : True\n",
      "original x rand : True\n",
      "original x major : True\n",
      "original x rnn : True\n",
      "original x transformer : True\n",
      "original x e2ernn : True\n",
      "original x e2etransformer : True\n",
      "original x melbourne : True\n",
      "original x upfforge : True\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mannwhitneyu, wilcoxon\n",
    "\n",
    "models = ['rand', 'major', 'rnn', 'transformer', 'e2ernn', 'e2etransformer', 'melbourne', 'upfforge', 'original']\n",
    "for i, model1 in enumerate(models):\n",
    "    for j, model2 in enumerate(models):\n",
    "        if model1 != model2:\n",
    "            fluency1 = [float(g['semantic']) for g in grades if g['model'] == model1]# and g['category'] in unseen_domains]\n",
    "            fluency2 = [float(g['semantic']) for g in grades if g['model'] == model2]# and g['category'] in unseen_domains]\n",
    "            print(model1, 'x', model2, ':', round(mannwhitneyu(fluency1, fluency2)[1], 2) < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_path='evaluation/questionaire/trials/gold.json'\n",
    "gold = json.load(open(gold_path))\n",
    "\n",
    "path='evaluation/questionaire/annotations.json'\n",
    "annotations = json.load(open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  models\\model1.xml\n",
      "Structured followed in 0.35 of the cases\n",
      "More information in 0.53 of the cases\n",
      "Exact number of predicates in 29 out of 75 of the cases (0.39)\n",
      "Reference mistakes in 0.21 of the cases\n",
      "Verb mistakes in 0.05 of the cases\n",
      "Determiner mistakes in 0.03 of the cases\n",
      "Fluency: 5.626666666666667\n",
      "Semantics: 3.52\n",
      "----------\n",
      "Model:  models\\model2.xml\n",
      "Structured followed in 0.33 of the cases\n",
      "More information in 0.41 of the cases\n",
      "Exact number of predicates in 35 out of 75 of the cases (0.47)\n",
      "Reference mistakes in 0.09 of the cases\n",
      "Verb mistakes in 0.03 of the cases\n",
      "Determiner mistakes in 0.0 of the cases\n",
      "Fluency: 4.546666666666667\n",
      "Semantics: 3.973333333333333\n",
      "----------\n",
      "Model:  models\\model3.xml\n",
      "Structured followed in 0.89 of the cases\n",
      "More information in 0.09 of the cases\n",
      "Exact number of predicates in 54 out of 75 of the cases (0.72)\n",
      "Reference mistakes in 0.28 of the cases\n",
      "Verb mistakes in 0.12 of the cases\n",
      "Determiner mistakes in 0.09 of the cases\n",
      "Fluency: 5.746666666666667\n",
      "Semantics: 5.64\n",
      "----------\n",
      "Model:  models\\model5.xml\n",
      "Structured followed in 0.41 of the cases\n",
      "More information in 0.05 of the cases\n",
      "Exact number of predicates in 32 out of 75 of the cases (0.43)\n",
      "Reference mistakes in 0.11 of the cases\n",
      "Verb mistakes in 0.05 of the cases\n",
      "Determiner mistakes in 0.09 of the cases\n",
      "Fluency: 3.7466666666666666\n",
      "Semantics: 3.3466666666666667\n",
      "----------\n",
      "Model:  models\\model7.xml\n",
      "Structured followed in 0.16 of the cases\n",
      "More information in 0.28 of the cases\n",
      "Exact number of predicates in 17 out of 25 of the cases (0.68)\n",
      "Reference mistakes in 0.4 of the cases\n",
      "Verb mistakes in 0.04 of the cases\n",
      "Determiner mistakes in 0.16 of the cases\n",
      "Fluency: 4.2\n",
      "Semantics: 3.92\n",
      "----------\n",
      "Model:  model1.xml\n",
      "Structured followed in 0.33 of the cases\n",
      "More information in 0.36 of the cases\n",
      "Exact number of predicates in 30 out of 75 of the cases (0.4)\n",
      "Reference mistakes in 0.13 of the cases\n",
      "Verb mistakes in 0.03 of the cases\n",
      "Determiner mistakes in 0.0 of the cases\n",
      "Fluency: 6.053333333333334\n",
      "Semantics: 3.6\n",
      "----------\n",
      "Model:  model3.xml\n",
      "Structured followed in 0.8 of the cases\n",
      "More information in 0.08 of the cases\n",
      "Exact number of predicates in 51 out of 75 of the cases (0.68)\n",
      "Reference mistakes in 0.07 of the cases\n",
      "Verb mistakes in 0.05 of the cases\n",
      "Determiner mistakes in 0.0 of the cases\n",
      "Fluency: 6.066666666666666\n",
      "Semantics: 5.253333333333333\n",
      "----------\n",
      "Model:  model4.xml\n",
      "Structured followed in 0.81 of the cases\n",
      "More information in 0.01 of the cases\n",
      "Exact number of predicates in 50 out of 75 of the cases (0.67)\n",
      "Reference mistakes in 0.2 of the cases\n",
      "Verb mistakes in 0.0 of the cases\n",
      "Determiner mistakes in 0.01 of the cases\n",
      "Fluency: 6.013333333333334\n",
      "Semantics: 4.933333333333334\n",
      "----------\n",
      "Model:  model6.xml\n",
      "Structured followed in 0.69 of the cases\n",
      "More information in 0.01 of the cases\n",
      "Exact number of predicates in 56 out of 75 of the cases (0.75)\n",
      "Reference mistakes in 0.01 of the cases\n",
      "Verb mistakes in 0.0 of the cases\n",
      "Determiner mistakes in 0.0 of the cases\n",
      "Fluency: 5.92\n",
      "Semantics: 5.413333333333333\n",
      "----------\n",
      "Model:  model8.xml\n",
      "Structured followed in 0.29 of the cases\n",
      "More information in 0.0 of the cases\n",
      "Exact number of predicates in 68 out of 75 of the cases (0.91)\n",
      "Reference mistakes in 0.0 of the cases\n",
      "Verb mistakes in 0.0 of the cases\n",
      "Determiner mistakes in 0.0 of the cases\n",
      "Fluency: 6.386666666666667\n",
      "Semantics: 6.48\n",
      "----------\n",
      "Model:  models\\model9.xml\n",
      "Structured followed in 0.39 of the cases\n",
      "More information in 0.12 of the cases\n",
      "Exact number of predicates in 74 out of 75 of the cases (0.99)\n",
      "Reference mistakes in 0.08 of the cases\n",
      "Verb mistakes in 0.05 of the cases\n",
      "Determiner mistakes in 0.05 of the cases\n",
      "Fluency: 6.8133333333333335\n",
      "Semantics: 6.866666666666666\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for model in annotations:\n",
    "    values = {\n",
    "        'detmistake': 0,\n",
    "        'fluency': [],\n",
    "        'moreinformation': 0,\n",
    "        'numpreds': 0,\n",
    "        'referencemistake': 0,\n",
    "        'semantics': [],\n",
    "        'structurefollowed': 0,\n",
    "        'verbmistake': 0\n",
    "    }\n",
    "    dem = 0\n",
    "    for trial in annotations[model]:\n",
    "        g = [g for g in gold if g['eid'] == trial][0]\n",
    "        category = g['category']\n",
    "        size = g['size']\n",
    "#         if category in unseen_domains:\n",
    "        if annotations[model][trial]['structurefollowed']:\n",
    "            values['structurefollowed'] += 1\n",
    "        if annotations[model][trial]['moreinformation']:\n",
    "            values['moreinformation'] += 1\n",
    "        if annotations[model][trial]['referencemistake']:\n",
    "            values['referencemistake'] += 1\n",
    "        if annotations[model][trial]['verbmistake']:\n",
    "            values['verbmistake'] += 1\n",
    "        if annotations[model][trial]['detmistake']:\n",
    "            values['detmistake'] += 1\n",
    "        if int(size) == int(annotations[model][trial]['numpreds']):\n",
    "            values['numpreds'] += 1\n",
    "        dem += 1\n",
    "        values['fluency'].append(float(annotations[model][trial]['fluency']))\n",
    "        values['semantics'].append(float(annotations[model][trial]['semantics']))\n",
    "    values['structurefollowed'] /= dem\n",
    "    values['moreinformation'] /= dem\n",
    "    values['referencemistake'] /= dem\n",
    "    values['verbmistake'] /= dem\n",
    "    values['detmistake'] /= dem\n",
    "\n",
    "    print('Model: ', model)\n",
    "    print('Structured followed in {0} of the cases'.format(round(values['structurefollowed'], 2)))\n",
    "    print('More information in {0} of the cases'.format(round(values['moreinformation'], 2)))\n",
    "    print('Exact number of predicates in {0} out of {1} of the cases ({2})'.format(values['numpreds'], dem, round(values['numpreds']/dem, 2)))\n",
    "    print('Reference mistakes in {0} of the cases'.format(round(values['referencemistake'], 2)))\n",
    "    print('Verb mistakes in {0} of the cases'.format(round(values['verbmistake'], 2)))\n",
    "    print('Determiner mistakes in {0} of the cases'.format(round(values['detmistake'], 2)))\n",
    "    print('Fluency: {0}'.format(np.mean(values['fluency'])))\n",
    "    print('Semantics: {0}'.format(np.mean(values['semantics'])))\n",
    "    print(10 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-annotator Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fluency:\n",
      "\n",
      "Weighted kappa as per NLTK:\t 0.261904761904762 \n",
      "Regular kappa as per NLTK:\t 0.2619047619047618 \n",
      "Krippendorff alpha as per NLTK:\t 0.2596645367412139 \n",
      "===========================================\n",
      "\n",
      "Semantic:\n",
      "\n",
      "Weighted kappa as per NLTK:\t 0.4459338695263628 \n",
      "Regular kappa as per NLTK:\t 0.4459338695263628 \n",
      "Krippendorff alpha as per NLTK:\t 0.44588252876998946 \n",
      "===========================================\n",
      "\n",
      "Predicates:\n",
      "\n",
      "Weighted kappa as per NLTK:\t 0.7042878265155249 \n",
      "Regular kappa as per NLTK:\t 0.7042878265155249 \n",
      "Krippendorff alpha as per NLTK:\t 0.7052170340955771 \n",
      "===========================================\n",
      "\n",
      "Structure Followed:\n",
      "\n",
      "Weighted kappa as per NLTK:\t 0.7796143250688705 \n",
      "Regular kappa as per NLTK:\t 0.7796143250688705 \n",
      "Krippendorff alpha as per NLTK:\t 0.7797016025050654 \n",
      "===========================================\n",
      "\n",
      "Overgeneration:\n",
      "\n",
      "Weighted kappa as per NLTK:\t 0.5279838165879973 \n",
      "Regular kappa as per NLTK:\t 0.5279838165879974 \n",
      "Krippendorff alpha as per NLTK:\t 0.5243181818181818 \n",
      "===========================================\n",
      "\n",
      "Verb mistakes:\n",
      "\n",
      "Weighted kappa as per NLTK:\t 0.6102449888641426 \n",
      "Regular kappa as per NLTK:\t 0.6102449888641432 \n",
      "Krippendorff alpha as per NLTK:\t 0.6079790222888182 \n",
      "===========================================\n",
      "\n",
      "Determiner mistakes:\n",
      "\n",
      "Weighted kappa as per NLTK:\t 0.0 \n",
      "Regular kappa as per NLTK:\t 0.0 \n",
      "Krippendorff alpha as per NLTK:\t -0.027491408934708028 \n",
      "===========================================\n",
      "\n",
      "Reference mistakes:\n",
      "\n",
      "Weighted kappa as per NLTK:\t 0.28251121076233177 \n",
      "Regular kappa as per NLTK:\t 0.282511210762332 \n",
      "Krippendorff alpha as per NLTK:\t 0.25806451612903225 \n",
      "===========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "\n",
    "def kappa(obs):\n",
    "    t = AnnotationTask(obs)\n",
    "    print(\"\\nWeighted kappa as per NLTK:\\t\", t.weighted_kappa(),\n",
    "          \"\\nRegular kappa as per NLTK:\\t\", t.kappa(),\n",
    "          \"\\nKrippendorff alpha as per NLTK:\\t\", t.alpha(),\n",
    "          \"\\n===========================================\\n\")\n",
    "\n",
    "path='evaluation/questionaire/observations/'\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "    \n",
    "ann1_model1 = annotations['models\\\\model1.xml']\n",
    "ann2_model1 = annotations['model1.xml']\n",
    "ann1_model3 = annotations['models\\\\model3.xml']\n",
    "ann2_model3 = annotations['model3.xml']\n",
    "\n",
    "obs_fluency = []\n",
    "for trial in ann1_model1:\n",
    "    obs_fluency.append(('ann1', 'model1_'+trial, int(ann1_model1[trial]['fluency'])))\n",
    "for trial in ann1_model3:\n",
    "    obs_fluency.append(('ann1', 'model3_'+trial, int(ann1_model3[trial]['fluency'])))\n",
    "for trial in ann2_model1:\n",
    "    obs_fluency.append(('ann2', 'model1_'+trial, int(ann2_model1[trial]['fluency'])))\n",
    "for trial in ann2_model3:\n",
    "    obs_fluency.append(('ann2', 'model3_'+trial, int(ann2_model3[trial]['fluency'])))\n",
    "\n",
    "print('Fluency:')\n",
    "kappa(obs_fluency)\n",
    "\n",
    "with open(os.path.join(path, 'fluency.csv'), 'w') as f:\n",
    "    for i, o in enumerate(obs_fluency):\n",
    "        obs_fluency[i] = list(obs_fluency[i])\n",
    "        obs_fluency[i][2] = str(obs_fluency[i][2])\n",
    "        obs_fluency[i] = ','.join(obs_fluency[i])\n",
    "    f.write('\\n'.join(obs_fluency))\n",
    "########################################################################################################\n",
    "obs_semantic = []\n",
    "for trial in ann1_model1:\n",
    "    obs_semantic.append(('ann1', 'model1_'+trial, int(ann1_model1[trial]['semantics'])))\n",
    "for trial in ann1_model3:\n",
    "    obs_semantic.append(('ann1', 'model3_'+trial, int(ann1_model3[trial]['semantics'])))\n",
    "for trial in ann2_model1:\n",
    "    obs_semantic.append(('ann2', 'model1_'+trial, int(ann2_model1[trial]['semantics'])))\n",
    "for trial in ann2_model3:\n",
    "    obs_semantic.append(('ann2', 'model3_'+trial, int(ann2_model3[trial]['semantics'])))\n",
    "\n",
    "print('Semantic:')\n",
    "kappa(obs_semantic)\n",
    "\n",
    "with open(os.path.join(path, 'semantic.csv'), 'w') as f:\n",
    "    for i, o in enumerate(obs_semantic):\n",
    "        obs_semantic[i] = list(obs_semantic[i])\n",
    "        obs_semantic[i][2] = str(obs_semantic[i][2])\n",
    "        obs_semantic[i] = ','.join(obs_semantic[i])\n",
    "    f.write('\\n'.join(obs_semantic))\n",
    "########################################################################################################\n",
    "obs_preds = []\n",
    "for trial in ann1_model1:\n",
    "    obs_preds.append(('1', 'model1_'+trial, int(ann1_model1[trial]['numpreds'])))\n",
    "for trial in ann1_model3:\n",
    "    obs_preds.append(('1', 'model3_'+trial, int(ann1_model3[trial]['numpreds'])))\n",
    "for trial in ann2_model1:\n",
    "    obs_preds.append(('2', 'model1_'+trial, int(ann2_model1[trial]['numpreds'])))\n",
    "for trial in ann2_model3:\n",
    "    obs_preds.append(('2', 'model3_'+trial, int(ann2_model3[trial]['numpreds'])))\n",
    "\n",
    "print('Predicates:')\n",
    "kappa(obs_preds)\n",
    "\n",
    "with open(os.path.join(path, 'predicates.csv'), 'w') as f:\n",
    "    for i, o in enumerate(obs_preds):\n",
    "        obs_preds[i] = list(obs_preds[i])\n",
    "        obs_preds[i][2] = str(obs_preds[i][2])\n",
    "        obs_preds[i] = ','.join(obs_preds[i])\n",
    "    f.write('\\n'.join(obs_preds))\n",
    "########################################################################################################\n",
    "obs = []\n",
    "for trial in ann1_model1:\n",
    "    obs.append(('1', 'model1_'+trial, 1 if ann1_model1[trial]['structurefollowed'] else 0))\n",
    "for trial in ann1_model3:\n",
    "    obs.append(('1', 'model3_'+trial, 1 if ann1_model3[trial]['structurefollowed'] else 0))\n",
    "for trial in ann2_model1:\n",
    "    obs.append(('2', 'model1_'+trial, 1 if ann2_model1[trial]['structurefollowed'] else 0))\n",
    "for trial in ann2_model3:\n",
    "    obs.append(('2', 'model3_'+trial, 1 if ann2_model3[trial]['structurefollowed'] else 0))\n",
    "\n",
    "print('Structure Followed:')\n",
    "kappa(obs)\n",
    "\n",
    "with open(os.path.join(path, 'structfollowed.csv'), 'w') as f:\n",
    "    for i, o in enumerate(obs):\n",
    "        obs[i] = list(obs[i])\n",
    "        obs[i][2] = str(obs[i][2])\n",
    "        obs[i] = ','.join(obs[i])\n",
    "    f.write('\\n'.join(obs))\n",
    "########################################################################################################\n",
    "obs = []\n",
    "for trial in ann1_model1:\n",
    "    obs.append(('1', 'model1_'+trial, 1 if ann1_model1[trial]['moreinformation'] else 0))\n",
    "for trial in ann1_model3:\n",
    "    obs.append(('1', 'model3_'+trial, 1 if ann1_model3[trial]['moreinformation'] else 0))\n",
    "for trial in ann2_model1:\n",
    "    obs.append(('2', 'model1_'+trial, 1 if ann2_model1[trial]['moreinformation'] else 0))\n",
    "for trial in ann2_model3:\n",
    "    obs.append(('2', 'model3_'+trial, 1 if ann2_model3[trial]['moreinformation'] else 0))\n",
    "\n",
    "print('Overgeneration:')\n",
    "kappa(obs)\n",
    "\n",
    "with open(os.path.join(path, 'overgeneration.csv'), 'w') as f:\n",
    "    for i, o in enumerate(obs):\n",
    "        obs[i] = list(obs[i])\n",
    "        obs[i][2] = str(obs[i][2])\n",
    "        obs[i] = ','.join(obs[i])\n",
    "    f.write('\\n'.join(obs))\n",
    "########################################################################################################\n",
    "obs = []\n",
    "for trial in ann1_model1:\n",
    "    obs.append(('1', 'model1_'+trial, 1 if ann1_model1[trial]['verbmistake'] else 0))\n",
    "for trial in ann1_model3:\n",
    "    obs.append(('1', 'model3_'+trial, 1 if ann1_model3[trial]['verbmistake'] else 0))\n",
    "for trial in ann2_model1:\n",
    "    obs.append(('2', 'model1_'+trial, 1 if ann2_model1[trial]['verbmistake'] else 0))\n",
    "for trial in ann2_model3:\n",
    "    obs.append(('2', 'model3_'+trial, 1 if ann2_model3[trial]['verbmistake'] else 0))\n",
    "\n",
    "print('Verb mistakes:')\n",
    "kappa(obs)\n",
    "\n",
    "with open(os.path.join(path, 'verbmistakes.csv'), 'w') as f:\n",
    "    for i, o in enumerate(obs):\n",
    "        obs[i] = list(obs[i])\n",
    "        obs[i][2] = str(obs[i][2])\n",
    "        obs[i] = ','.join(obs[i])\n",
    "    f.write('\\n'.join(obs))\n",
    "########################################################################################################\n",
    "obs = []\n",
    "for trial in ann1_model1:\n",
    "    obs.append(('1', 'model1_'+trial, 1 if ann1_model1[trial]['detmistake'] else 0))\n",
    "for trial in ann1_model3:\n",
    "    obs.append(('1', 'model3_'+trial, 1 if ann1_model3[trial]['detmistake'] else 0))\n",
    "for trial in ann2_model1:\n",
    "    obs.append(('2', 'model1_'+trial, 1 if ann2_model1[trial]['detmistake'] else 0))\n",
    "for trial in ann2_model3:\n",
    "    obs.append(('2', 'model3_'+trial, 1 if ann2_model3[trial]['detmistake'] else 0))\n",
    "\n",
    "print('Determiner mistakes:')\n",
    "kappa(obs)\n",
    "\n",
    "with open(os.path.join(path, 'detmistakes.csv'), 'w') as f:\n",
    "    for i, o in enumerate(obs):\n",
    "        obs[i] = list(obs[i])\n",
    "        obs[i][2] = str(obs[i][2])\n",
    "        obs[i] = ','.join(obs[i])\n",
    "    f.write('\\n'.join(obs))\n",
    "########################################################################################################\n",
    "obs = []\n",
    "for trial in ann1_model1:\n",
    "    obs.append(('1', 'model1_'+trial, 1 if ann1_model1[trial]['referencemistake'] else 0))\n",
    "for trial in ann1_model3:\n",
    "    obs.append(('1', 'model3_'+trial, 1 if ann1_model3[trial]['referencemistake'] else 0))\n",
    "for trial in ann2_model1:\n",
    "    obs.append(('2', 'model1_'+trial, 1 if ann2_model1[trial]['referencemistake'] else 0))\n",
    "for trial in ann2_model3:\n",
    "    obs.append(('2', 'model3_'+trial, 1 if ann2_model3[trial]['referencemistake'] else 0))\n",
    "\n",
    "print('Reference mistakes:')\n",
    "kappa(obs)\n",
    "\n",
    "with open(os.path.join(path, 'refmistakes.csv'), 'w') as f:\n",
    "    for i, o in enumerate(obs):\n",
    "        obs[i] = list(obs[i])\n",
    "        obs[i][2] = str(obs[i][2])\n",
    "        obs[i] = ','.join(obs[i])\n",
    "    f.write('\\n'.join(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_triples(text):\n",
    "    triples, triple = [], []\n",
    "    for w in text:\n",
    "        if w not in ['<TRIPLE>', '</TRIPLE>']:\n",
    "            triple.append(w)\n",
    "        elif w == '</TRIPLE>':\n",
    "            triples.append(triple)\n",
    "            triple = []\n",
    "    return triples\n",
    "\n",
    "def ordering_analysis(ordering, gold):\n",
    "    for i, entry in enumerate(gold):\n",
    "        triples = split_triples(entry['source'])\n",
    "\n",
    "        num, visited = 0, []\n",
    "        for triple in triples:\n",
    "            for j, predicate in enumerate(ordering[i]):\n",
    "                if predicate == triple[1] and j not in visited:\n",
    "                    num += 1\n",
    "                    visited.append(j)\n",
    "        # How many predicates in the modified tripleset are present in the result?\n",
    "        entry['ordering'] = num\n",
    "    return gold\n",
    "\n",
    "\n",
    "def structing_analysis(structing, gold):\n",
    "    for i, entry in enumerate(self.gold):\n",
    "        triples = split_triples(entry['source'])\n",
    "\n",
    "        num, visited = 0, []\n",
    "        for triple in triples:\n",
    "            for j, predicate in enumerate(structing[i]):\n",
    "                if predicate == triple[1] and j not in visited:\n",
    "                    num += 1\n",
    "                    visited.append(j)\n",
    "        # How many predicates in the modified tripleset are present in the result?\n",
    "        entry['structing'] = num\n",
    "    return gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact number of predicates in 52 out of 75 of the cases (0.69)\n"
     ]
    }
   ],
   "source": [
    "path = 'results/questionaire/partial.txt'\n",
    "with open(path) as f:\n",
    "    eids = f.read().split()\n",
    "    \n",
    "gold_path='evaluation/questionaire/gold.json'\n",
    "gold = json.load(open(gold_path))\n",
    "\n",
    "p = 'evaluation/results/pipeline/transformer/test.structing.postprocessed'\n",
    "with open(p) as f:\n",
    "    ordering = f.read().split('\\n')[:-1]\n",
    "\n",
    "pos, dem = 0, 0\n",
    "for i, entry in enumerate(gold):\n",
    "    if entry['eid'] in eids:\n",
    "        category = entry['category']\n",
    "#         if category in unseen_domains:\n",
    "        triples = split_triples(entry['source'])\n",
    "        num, visited = 0, []\n",
    "        for triple in triples:\n",
    "            for j, predicate in enumerate(ordering[i].split()):\n",
    "                if predicate == triple[1] and j not in visited:\n",
    "                    num += 1\n",
    "                    visited.append(j)\n",
    "                if len(visited) == len(triples):\n",
    "                    break\n",
    "        if len(triples) == len(visited):\n",
    "            pos += 1\n",
    "        dem += 1\n",
    "\n",
    "print('Exact number of predicates in {0} out of {1} of the cases ({2})'.format(pos, dem, round(pos/dem, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
