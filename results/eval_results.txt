EVALUATION RESULTS FOR DATA-TO-TEXT WEBNLG 

Evaluation of Ordering and Structuring 

All domains:  

Task: ordering
Set: test
Model: t5
Accuracy: 0.53
--------------------

Task: ordering
Set: test
Model: bart
Accuracy: 0.36
--------------------

Task: ordering
Set: test
Model: gpt2
Accuracy: 0.37
--------------------

Task: ordering
Set: test
Model: chatgpt
Accuracy: 0.3
--------------------

Task: structuring
Set: test
Model: t5
Accuracy: 0.65
--------------------

Task: structuring
Set: test
Model: bart
Accuracy: 0.32
--------------------

Task: structuring
Set: test
Model: gpt2
Accuracy: 0.4
--------------------

Task: structuring
Set: test
Model: chatgpt
Accuracy: 0.49
--------------------

Task: ordering
Set: dev
Model: t5
Accuracy: 0.64
--------------------

Task: ordering
Set: dev
Model: bart
Accuracy: 0.57
--------------------

Task: ordering
Set: dev
Model: gpt2
Accuracy: 0.6
--------------------

Task: ordering
Set: dev
Model: chatgpt
Accuracy: 0.26
--------------------

Task: structuring
Set: dev
Model: t5
Accuracy: 0.68
--------------------

Task: structuring
Set: dev
Model: bart
Accuracy: 0.48
--------------------

Task: structuring
Set: dev
Model: gpt2
Accuracy: 0.67
--------------------

Task: structuring
Set: dev
Model: chatgpt
Accuracy: 0.53
--------------------

############################################################

Seen domains:  

Task: ordering
Set: test
Model: t5
Accuracy: 0.64
--------------------

Task: ordering
Set: test
Model: bart
Accuracy: 0.56
--------------------

Task: ordering
Set: test
Model: gpt2
Accuracy: 0.58
--------------------

Task: ordering
Set: test
Model: chatgpt
Accuracy: 0.28
--------------------

Task: structuring
Set: test
Model: t5
Accuracy: 0.68
--------------------

Task: structuring
Set: test
Model: bart
Accuracy: 0.45
--------------------

Task: structuring
Set: test
Model: gpt2
Accuracy: 0.63
--------------------

Task: structuring
Set: test
Model: chatgpt
Accuracy: 0.51
--------------------

Task: ordering
Set: dev
Model: t5
Accuracy: 0.64
--------------------

Task: ordering
Set: dev
Model: bart
Accuracy: 0.57
--------------------

Task: ordering
Set: dev
Model: gpt2
Accuracy: 0.6
--------------------

Task: ordering
Set: dev
Model: chatgpt
Accuracy: 0.26
--------------------

Task: structuring
Set: dev
Model: t5
Accuracy: 0.68
--------------------

Task: structuring
Set: dev
Model: bart
Accuracy: 0.48
--------------------

Task: structuring
Set: dev
Model: gpt2
Accuracy: 0.67
--------------------

Task: structuring
Set: dev
Model: chatgpt
Accuracy: 0.53
--------------------

############################################################

Unseen domains:  

Task: ordering
Set: test
Model: t5
Accuracy: 0.4
--------------------

Task: ordering
Set: test
Model: bart
Accuracy: 0.14
--------------------

Task: ordering
Set: test
Model: gpt2
Accuracy: 0.12
--------------------

Task: ordering
Set: test
Model: chatgpt
Accuracy: 0.33
--------------------

Task: structuring
Set: test
Model: t5
Accuracy: 0.63
--------------------

Task: structuring
Set: test
Model: bart
Accuracy: 0.18
--------------------

Task: structuring
Set: test
Model: gpt2
Accuracy: 0.16
--------------------

Task: structuring
Set: test
Model: chatgpt
Accuracy: 0.46
--------------------

Task: ordering
Set: dev
Model: t5
Accuracy: 0
--------------------

Task: ordering
Set: dev
Model: bart
Accuracy: 0
--------------------

Task: ordering
Set: dev
Model: gpt2
Accuracy: 0
--------------------

Task: ordering
Set: dev
Model: chatgpt
Accuracy: 0
--------------------

Task: structuring
Set: dev
Model: t5
Accuracy: 0
--------------------

Task: structuring
Set: dev
Model: bart
Accuracy: 0
--------------------

Task: structuring
Set: dev
Model: gpt2
Accuracy: 0
--------------------

Task: structuring
Set: dev
Model: chatgpt
Accuracy: 0
--------------------

############################################################


Evaluation of Referring Expressions 

All domains:  

Task: REG
Set: test
Model: t5
Baseline Accuracy: 0.51
Accuracy: 0.66
--------------------

Task: REG
Set: test
Model: bart
Baseline Accuracy: 0.51
Accuracy: 0.42
--------------------

Task: REG
Set: test
Model: gpt2
Baseline Accuracy: 0.51
Accuracy: 0.39
--------------------

Task: REG
Set: dev
Model: t5
Baseline Accuracy: 0.54
Accuracy: 0.74
--------------------

Task: REG
Set: dev
Model: bart
Baseline Accuracy: 0.54
Accuracy: 0.63
--------------------

Task: REG
Set: dev
Model: gpt2
Baseline Accuracy: 0.54
Accuracy: 0.63
--------------------

############################################################

Seen domains:  

Task: REG
Set: test
Model: t5
Baseline Accuracy: 0.53
Accuracy: 0.74
--------------------

Task: REG
Set: test
Model: bart
Baseline Accuracy: 0.53
Accuracy: 0.64
--------------------

Task: REG
Set: test
Model: gpt2
Baseline Accuracy: 0.53
Accuracy: 0.64
--------------------

Task: REG
Set: dev
Model: t5
Baseline Accuracy: 0.54
Accuracy: 0.74
--------------------

Task: REG
Set: dev
Model: bart
Baseline Accuracy: 0.54
Accuracy: 0.63
--------------------

Task: REG
Set: dev
Model: gpt2
Baseline Accuracy: 0.54
Accuracy: 0.63
--------------------

############################################################

Unseen domains:  

Task: REG
Set: test
Model: t5
Baseline Accuracy: 0.5
Accuracy: 0.58
--------------------

Task: REG
Set: test
Model: bart
Baseline Accuracy: 0.5
Accuracy: 0.18
--------------------

Task: REG
Set: test
Model: gpt2
Baseline Accuracy: 0.5
Accuracy: 0.13
--------------------

Task: REG
Set: dev
Model: t5
Baseline Accuracy: 0
Accuracy: 0
--------------------

Task: REG
Set: dev
Model: bart
Baseline Accuracy: 0
Accuracy: 0
--------------------

Task: REG
Set: dev
Model: gpt2
Baseline Accuracy: 0
Accuracy: 0
--------------------

############################################################


Evaluation of Lexicalization 

All domains:  

Task: Lexicalization
Set: test
Model: t5
Result: 0.43
--------------------

Task: Lexicalization
Set: test
Model: bart
Result: 0.2
--------------------

Task: Lexicalization
Set: test
Model: gpt2
Result: 0.42
--------------------

Task: Lexicalization
Set: dev
Model: t5
Result: 0.45
--------------------

Task: Lexicalization
Set: dev
Model: bart
Result: 0.21
--------------------

Task: Lexicalization
Set: dev
Model: gpt2
Result: 0.45
--------------------

############################################################

Seen domains:  

Task: Lexicalization
Set: test
Model: t5
Result: 0.45
--------------------

Task: Lexicalization
Set: test
Model: bart
Result: 0.2
--------------------

Task: Lexicalization
Set: test
Model: gpt2
Result: 0.45
--------------------

Task: Lexicalization
Set: dev
Model: t5
Result: 0.45
--------------------

Task: Lexicalization
Set: dev
Model: bart
Result: 0.21
--------------------

Task: Lexicalization
Set: dev
Model: gpt2
Result: 0.45
--------------------

############################################################

Unseen domains:  

Task: Lexicalization
Set: test
Model: t5
Result: 0.41
--------------------

Task: Lexicalization
Set: test
Model: bart
Result: 0.19
--------------------

Task: Lexicalization
Set: test
Model: gpt2
Result: 0.38
--------------------

Task: Lexicalization
Set: dev
Model: t5
Result: 0
--------------------

Task: Lexicalization
Set: dev
Model: bart
Result: 0
--------------------

Task: Lexicalization
Set: dev
Model: gpt2
Result: 0
--------------------

############################################################


Evaluation of Final Texts (BLEU)

All domains:

Task: end2end
Set: test
Model: t5
Result: 0.56
--------------------

Task: end2end
Set: test
Model: bart
Result: 0.45
--------------------

Task: end2end
Set: test
Model: gpt2
Result: 0.33
--------------------

Task: sr
Set: test
Model: t5
Result: 0.46
--------------------

Task: sr
Set: test
Model: bart
Result: 0.21
--------------------

Task: sr
Set: test
Model: gpt2
Result: 0.32
--------------------

Task: end2end
Set: eval
Model: t5
Result: 0.65
--------------------

Task: end2end
Set: eval
Model: bart
Result: 0.64
--------------------

Task: end2end
Set: eval
Model: gpt2
Result: 0.62
--------------------

Task: sr
Set: eval
Model: t5
Result: 0.55
--------------------

Task: sr
Set: eval
Model: bart
Result: 0.28
--------------------

Task: sr
Set: eval
Model: gpt2
Result: 0.49
--------------------

############################################################

Seen domains:

Task: end2end
Set: test
Model: t5
Result: 0.63
--------------------

Task: end2end
Set: test
Model: bart
Result: 0.62
--------------------

Task: end2end
Set: test
Model: gpt2
Result: 0.58
--------------------

Task: sr
Set: test
Model: t5
Result: 0.53
--------------------

Task: sr
Set: test
Model: bart
Result: 0.27
--------------------

Task: sr
Set: test
Model: gpt2
Result: 0.47
--------------------

Task: end2end
Set: eval
Model: t5
Result: 0.65
--------------------

Task: end2end
Set: eval
Model: bart
Result: 0.64
--------------------

Task: end2end
Set: eval
Model: gpt2
Result: 0.62
--------------------

Task: sr
Set: eval
Model: t5
Result: 0.55
--------------------

Task: sr
Set: eval
Model: bart
Result: 0.28
--------------------

Task: sr
Set: eval
Model: gpt2
Result: 0.49
--------------------

############################################################

Unseen domains:

Task: end2end
Set: test
Model: t5
Result: 0.46
--------------------

Task: end2end
Set: test
Model: bart
Result: 0.26
--------------------

Task: end2end
Set: test
Model: gpt2
Result: 0.13
--------------------

Task: sr
Set: test
Model: t5
Result: 0.37
--------------------

Task: sr
Set: test
Model: bart
Result: 0.13
--------------------

Task: sr
Set: test
Model: gpt2
Result: 0.13
--------------------

Task: end2end
Set: eval
Model: t5
Result: 0
--------------------

Task: end2end
Set: eval
Model: bart
Result: 0
--------------------

Task: end2end
Set: eval
Model: gpt2
Result: 0
--------------------

Task: sr
Set: eval
Model: t5
Result: 0
--------------------

Task: sr
Set: eval
Model: bart
Result: 0
--------------------

Task: sr
Set: eval
Model: gpt2
Result: 0
--------------------

############################################################


Evaluation of Final Texts (METEOR)

All domains:
