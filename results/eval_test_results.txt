Evaluation of Pipeline Neural Architecture Using Webnlg dataset on LLM's
Evaluation of pipeline Ordering and Structuring task

Task: ordering
Set: dev
Model: t5
Domain: All domains
Accuracy: 0.64
--------------------

Task: ordering
Set: dev
Model: bart
Domain: All domains
Accuracy: 0.57
--------------------

Task: ordering
Set: dev
Model: gpt2
Domain: All domains
Accuracy: 0.6
--------------------

Task: ordering
Set: dev
Model: t5
Domain: Seen domains
Accuracy: 0.64
--------------------

Task: ordering
Set: dev
Model: bart
Domain: Seen domains
Accuracy: 0.57
--------------------

Task: ordering
Set: dev
Model: gpt2
Domain: Seen domains
Accuracy: 0.6
--------------------

Task: ordering
Set: dev
Model: t5
Domain: Unseen domains
Accuracy: 0.0
--------------------

Task: ordering
Set: dev
Model: bart
Domain: Unseen domains
Accuracy: 0.0
--------------------

Task: ordering
Set: dev
Model: gpt2
Domain: Unseen domains
Accuracy: 0.0
--------------------

Task: structuring
Set: dev
Model: t5
Domain: All domains
Accuracy: 0.68
--------------------

Task: structuring
Set: dev
Model: bart
Domain: All domains
Accuracy: 0.48
--------------------

Task: structuring
Set: dev
Model: gpt2
Domain: All domains
Accuracy: 0.67
--------------------

Task: structuring
Set: dev
Model: t5
Domain: Seen domains
Accuracy: 0.68
--------------------

Task: structuring
Set: dev
Model: bart
Domain: Seen domains
Accuracy: 0.48
--------------------

Task: structuring
Set: dev
Model: gpt2
Domain: Seen domains
Accuracy: 0.67
--------------------

Task: structuring
Set: dev
Model: t5
Domain: Unseen domains
Accuracy: 0.0
--------------------

Task: structuring
Set: dev
Model: bart
Domain: Unseen domains
Accuracy: 0.0
--------------------

Task: structuring
Set: dev
Model: gpt2
Domain: Unseen domains
Accuracy: 0.0
--------------------

Task: ordering
Set: test
Model: t5
Domain: All domains
Accuracy: 0.53
--------------------

Task: ordering
Set: test
Model: bart
Domain: All domains
Accuracy: 0.36
--------------------

Task: ordering
Set: test
Model: gpt2
Domain: All domains
Accuracy: 0.37
--------------------

Task: ordering
Set: test
Model: t5
Domain: Seen domains
Accuracy: 0.64
--------------------

Task: ordering
Set: test
Model: bart
Domain: Seen domains
Accuracy: 0.56
--------------------

Task: ordering
Set: test
Model: gpt2
Domain: Seen domains
Accuracy: 0.58
--------------------

Task: ordering
Set: test
Model: t5
Domain: Unseen domains
Accuracy: 0.4
--------------------

Task: ordering
Set: test
Model: bart
Domain: Unseen domains
Accuracy: 0.14
--------------------

Task: ordering
Set: test
Model: gpt2
Domain: Unseen domains
Accuracy: 0.12
--------------------

Task: structuring
Set: test
Model: t5
Domain: All domains
Accuracy: 0.65
--------------------

Task: structuring
Set: test
Model: bart
Domain: All domains
Accuracy: 0.32
--------------------

Task: structuring
Set: test
Model: gpt2
Domain: All domains
Accuracy: 0.4
--------------------

Task: structuring
Set: test
Model: t5
Domain: Seen domains
Accuracy: 0.68
--------------------

Task: structuring
Set: test
Model: bart
Domain: Seen domains
Accuracy: 0.45
--------------------

Task: structuring
Set: test
Model: gpt2
Domain: Seen domains
Accuracy: 0.63
--------------------

Task: structuring
Set: test
Model: t5
Domain: Unseen domains
Accuracy: 0.63
--------------------

Task: structuring
Set: test
Model: bart
Domain: Unseen domains
Accuracy: 0.18
--------------------

Task: structuring
Set: test
Model: gpt2
Domain: Unseen domains
Accuracy: 0.16
--------------------

############################################################

Evaluation of Lexicalization task

Task: Lexicalization
Set: dev
Model: t5
Domain: All domains
Result: 0.45
--------------------

Task: Lexicalization
Set: dev
Model: bart
Domain: All domains
Result: 0.21
--------------------

Task: Lexicalization
Set: dev
Model: gpt2
Domain: All domains
Result: 0.45
--------------------

Task: Lexicalization
Set: dev
Model: t5
Domain: Seen domains
Result: 0.45
--------------------

Task: Lexicalization
Set: dev
Model: bart
Domain: Seen domains
Result: 0.21
--------------------

Task: Lexicalization
Set: dev
Model: gpt2
Domain: Seen domains
Result: 0.45
--------------------

Task: Lexicalization
Set: dev
Model: t5
Domain: Unseen domains
Result: 0
--------------------

Task: Lexicalization
Set: dev
Model: bart
Domain: Unseen domains
Result: 0
--------------------

Task: Lexicalization
Set: dev
Model: gpt2
Domain: Unseen domains
Result: 0
--------------------

Task: Lexicalization
Set: test
Model: t5
Domain: All domains
Result: 0.43
--------------------

Task: Lexicalization
Set: test
Model: bart
Domain: All domains
Result: 0.2
--------------------

Task: Lexicalization
Set: test
Model: gpt2
Domain: All domains
Result: 0.42
--------------------

Task: Lexicalization
Set: test
Model: t5
Domain: Seen domains
Result: 0.45
--------------------

Task: Lexicalization
Set: test
Model: bart
Domain: Seen domains
Result: 0.2
--------------------

Task: Lexicalization
Set: test
Model: gpt2
Domain: Seen domains
Result: 0.45
--------------------

Task: Lexicalization
Set: test
Model: t5
Domain: Unseen domains
Result: 0.41
--------------------

Task: Lexicalization
Set: test
Model: bart
Domain: Unseen domains
Result: 0.19
--------------------

Task: Lexicalization
Set: test
Model: gpt2
Domain: Unseen domains
Result: 0.38
--------------------

############################################################

Evaluation of pipeline reg task

Task: REG
Set: dev
Model: t5
Domain: All domains
Baseline Accuracy: 0.54
Accuracy: 0.74
--------------------

Task: REG
Set: dev
Model: bart
Domain: All domains
Baseline Accuracy: 0.54
Accuracy: 0.63
--------------------

Task: REG
Set: dev
Model: gpt2
Domain: All domains
Baseline Accuracy: 0.54
Accuracy: 0.63
--------------------

Task: REG
Set: dev
Model: t5
Domain: Seen domains
Baseline Accuracy: 0.54
Accuracy: 0.74
--------------------

Task: REG
Set: dev
Model: bart
Domain: Seen domains
Baseline Accuracy: 0.54
Accuracy: 0.63
--------------------

Task: REG
Set: dev
Model: gpt2
Domain: Seen domains
Baseline Accuracy: 0.54
Accuracy: 0.63
--------------------

Task: REG
Set: dev
Model: t5
Domain: Unseen domains
Baseline Accuracy: 0
Accuracy: 0
--------------------

Task: REG
Set: dev
Model: bart
Domain: Unseen domains
Baseline Accuracy: 0
Accuracy: 0
--------------------

Task: REG
Set: dev
Model: gpt2
Domain: Unseen domains
Baseline Accuracy: 0
Accuracy: 0
--------------------

Task: REG
Set: test
Model: t5
Domain: All domains
Baseline Accuracy: 0.51
Accuracy: 0.66
--------------------

Task: REG
Set: test
Model: bart
Domain: All domains
Baseline Accuracy: 0.51
Accuracy: 0.42
--------------------

Task: REG
Set: test
Model: gpt2
Domain: All domains
Baseline Accuracy: 0.51
Accuracy: 0.39
--------------------

Task: REG
Set: test
Model: t5
Domain: Seen domains
Baseline Accuracy: 0.53
Accuracy: 0.74
--------------------

Task: REG
Set: test
Model: bart
Domain: Seen domains
Baseline Accuracy: 0.53
Accuracy: 0.64
--------------------

Task: REG
Set: test
Model: gpt2
Domain: Seen domains
Baseline Accuracy: 0.53
Accuracy: 0.64
--------------------

Task: REG
Set: test
Model: t5
Domain: Unseen domains
Baseline Accuracy: 0.5
Accuracy: 0.58
--------------------

Task: REG
Set: test
Model: bart
Domain: Unseen domains
Baseline Accuracy: 0.5
Accuracy: 0.18
--------------------

Task: REG
Set: test
Model: gpt2
Domain: Unseen domains
Baseline Accuracy: 0.5
Accuracy: 0.13
--------------------

############################################################

Evaluation of Bleu Score for pipeline End2end and Surface Realization task

Task: end2end
Set: dev
Model: gpt2
Domain: All domains
Result_bleu: 0.62
--------------------

Task: end2end
Set: dev
Model: gpt2
Domain: Seen domains
Result_bleu: 0.62
--------------------

Task: end2end
Set: dev
Model: gpt2
Domain: Unseen domains
Result_bleu: 0
--------------------

Task: sr
Set: dev
Model: gpt2
Domain: All domains
Result_bleu: 0.49
--------------------

Task: sr
Set: dev
Model: gpt2
Domain: Seen domains
Result_bleu: 0.49
--------------------

Task: sr
Set: dev
Model: gpt2
Domain: Unseen domains
Result_bleu: 0
--------------------

Task: end2end
Set: test
Model: gpt2
Domain: All domains
Result_bleu: 0.33
--------------------

Task: end2end
Set: test
Model: gpt2
Domain: Seen domains
Result_bleu: 0.58
--------------------

Task: end2end
Set: test
Model: gpt2
Domain: Unseen domains
Result_bleu: 0.13
--------------------

Task: sr
Set: test
Model: gpt2
Domain: All domains
Result_bleu: 0.32
--------------------

Task: sr
Set: test
Model: gpt2
Domain: Seen domains
Result_bleu: 0.47
--------------------

Task: sr
Set: test
Model: gpt2
Domain: Unseen domains
Result_bleu: 0.13
--------------------

############################################################

Evaluation of Meteor Score for pipeline End2end and Surface Realization task

Task: end2end
Set: dev
Model: gpt2
Domain: All domains
Result_Meteor: 0.37
--------------------

Task: end2end
Set: dev
Model: gpt2
Domain: Seen domains
Result_Meteor: 0.37
--------------------

Task: end2end
Set: dev
Model: gpt2
Domain: Unseen domains
Result_Meteor: 0.0
--------------------

Task: sr
Set: dev
Model: gpt2
Domain: All domains
Result_Meteor: 0.31
--------------------

Task: sr
Set: dev
Model: gpt2
Domain: Seen domains
Result_Meteor: 0.31
--------------------

Task: sr
Set: dev
Model: gpt2
Domain: Unseen domains
Result_Meteor: 0.0
--------------------

Task: end2end
Set: test
Model: gpt2
Domain: All domains
Result_Meteor: 0.3
--------------------

Task: end2end
Set: test
Model: gpt2
Domain: Seen domains
Result_Meteor: 0.37
--------------------

Task: end2end
Set: test
Model: gpt2
Domain: Unseen domains
Result_Meteor: 0.22
--------------------

Task: sr
Set: test
Model: gpt2
Domain: All domains
Result_Meteor: 0.23
--------------------

Task: sr
Set: test
Model: gpt2
Domain: Seen domains
Result_Meteor: 0.3
--------------------

Task: sr
Set: test
Model: gpt2
Domain: Unseen domains
Result_Meteor: 0.14
--------------------

############################################################
